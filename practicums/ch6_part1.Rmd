---
title: "ch6"
author: "mht"
date: "October 21, 2014"
output: html_document
---

# Chapter 6: Latent Mixture Models

Suppose a group of 15 people sit an exam made up of 40 true-or-false questions, and they get 21, 17, 21, 18, 22, 31, 31, 34, 34, 35, 35, 36, 39, 36, and 35 right. These scores suggest that the first 5 people were just guessing, but the last 10 had some level of knowledge.

One way to make statistical inferences along these lines is to assume there are two different groups of people. These groups have different probabilities of success, with the guessing group having a probability of 0.5, and the knowledge group having a probability greater than 0.5. Whether each person belongs to the first or the second group is a latent or unobserved variable that can take just two values. Using this approach, the goal is to infer to which group each person belongs, and also the rate of success for the knowledge group.

This type of model is known as a latent-mixture model, because the data are assumed to be generated by two different processes that combine or mix, and im- portant properties of that mixture are unobserved or latent. In this case, the two components that mix are the guessing and knowledge processes, and the group membership of each person is latent.

Recall that in WebPPL, `uniform(0.5,1)` is the same as `sample(Uniform({a:0.5, b:1}))`. Also, `flip(0.5)` is shorthand for `bernoulli(0.5)`.

```{r 6.1.1,fig.width=10, fig.height=4}
library(rwebppl)
library(ggplot2)
library(tidyr)
library(dplyr)

#setwd("~/Repos/psych201s/practicums")

exam.scores <- '
// unpack data
var n_correct = observed_data["n_correct"];
var n_people = observed_data["n_people"];
var n_questions = observed_data["n_questions"];

var model = function() {
  
  var knowledgeSuccessRate = uniform(0.5, 1)
  var guessingSuccessRate = 0.5  

  foreach(_.range(0, n_people), function(id){

    var group =  flip(0.5) ? "knowledge" : "guessing";
    var successRate = group=="knowledge" ? 
                        knowledgeSuccessRate : 
                        guessingSuccessRate

    observe({data: n_correct[id], 
              link: Binomial({n:n_questions, p:successRate})})
    
    query.add("p" + id, group)

  })

  query.add("knowledgeRate", knowledgeSuccessRate)

  return query
}
'

k <- c(21,17,21,18,22,31,31,34,34,35,35,36,39,36,35)

observed_data = list(
  n_correct = k,
  n_questions = 40,
  n_people = length(k)
)

numSamples = 50000;

wp.samp <- webppl(program_code = exam.scores,
       data = observed_data,
       data_var = "observed_data",
       inference_opts = list(method="MCMC", 
                          samples = numSamples, 
                          burn = numSamples/2,
                          verbose = TRUE),
      model_var = "model",
      packages = c("./utils"),
      output_format = "samples"
      )

# As always, first look at what your output looks like:

head(wp.samp)

# plot the inferred "knowledge rate"
ggplot(wp.samp, aes(x= knowledgeRate)) + 
  geom_histogram(binwidth = 0.01)+
  xlim(0.5,1)

# tidy the data for looking at the "group" variable
wp.tidy <- wp.samp %>% 
  select(-knowledgeRate) %>%
  gather(participant, group) %>%
  mutate(participant = factor(participant,
                              levels = paste("p",0:length(k)-1, sep = "")))

# plot posterior on group variable for each participant
ggplot(wp.tidy, aes(x = group))+
  geom_bar() + 
  facet_wrap(~participant)
```

### Ex 6.1.1

Draw some conclusions about the problem from the posterior distribution.
Who belongs to what group, and how confident are you?

### SKIP Ex 6.1.2

### Ex 6.1.3 

Include an extra person in the exam, with a score of 28 out of 40. What does their posterior for group tell you? 
...

Now add four extra people, all with the score 28 out of 40. Explain the change these extra people make to the inference.
...

```{r 6.1.3a,fig.width=10, fig.height=4, echo=FALSE}
k <- c(21,17,21,18,22,31,31,34,34,35,35,36,39,36,35,28)

observed_data = list(
  n_correct = k,
  n_questions = 40,
  n_people = length(k)
)

wp.samp <- webppl(program_code = exam.scores,
       data = observed_data,
       data_var = "observed_data",
       inference_opts = list(method="MCMC", 
                          samples = numSamples, 
                          burn = numSamples/2,
                          verbose = TRUE),
      model_var = "model",
      packages = c("./utils"),
      output_format = "samples"
      )


# plot the inferred "knowledge rate"
ggplot(wp.samp, aes(x= knowledgeRate)) + 
  geom_histogram(binwidth = 0.01)+
  xlim(0.5,1)

wp.tidy <- wp.samp %>% 
  select(-knowledgeRate) %>%
  gather(participant, group) %>%
  mutate(participant = factor(participant,
                              levels = paste("p",0:length(k)-1, sep = "")))

ggplot(wp.tidy, aes(x = group))+
  geom_bar() + 
  facet_wrap(~participant)
```


### Ex. 6.1.4 

What happens if you change the prior on the success rate of the second group to be uniform over the whole range from 0 to 1, and so allow for worse-than-guessing performance?


```{r 6.1.4,fig.width=10, fig.height=4}
exam.scores2 <- '
// unpack data
var n_correct = observed_data["n_correct"];
var n_people = observed_data["n_people"];
var n_questions = observed_data["n_questions"];


var model = function() {
  
  var knowledgeSuccessRate = uniform(0.5, 1)
  var guessingSuccessRate = uniform(0,1)//0.5  

  foreach(_.range(0, n_people), function(id){

    var group =  flip(0.5) ? "knowledge" : "guessing";
    var successRate = group=="knowledge" ? 
                        knowledgeSuccessRate : 
                        guessingSuccessRate

    observe({data: n_correct[id], 
              link: Binomial({n:n_questions, p:successRate})})
    
    query.add("p" + id, group)

  })

  query.add("knowledgeRate", knowledgeSuccessRate)
  query.add("guessingRate", guessingSuccessRate)

  return query
}
'

k <- c(21,17,21,18,22,31,31,34,34,35,35,36,39,36,35, 28, 28, 28, 28)

observed_data = list(
  n_correct = k,
  n_questions = 40,
  n_people = length(k)
)

numSamples = 50000;

wp.samp <- webppl(program_code = exam.scores2,
       data = observed_data,
       data_var = "observed_data",
       inference_opts = list(method="MCMC", 
                          samples = numSamples, 
                          burn = numSamples/2,
                          verbose = TRUE),
      model_var = "model",
      packages = c("./utils"),
      output_format = "samples"
      )


wp.samp %>% select(knowledgeRate, guessingRate) %>%
  gather(parameter, value) %>%
  ggplot(., aes(x= value)) + 
  geom_histogram()+
  facet_wrap(~parameter)

wp.tidy <- wp.samp %>% 
  select(-knowledgeRate, -guessingRate) %>%
  gather(participant, group) %>%
  mutate(participant = factor(participant,
                              levels = paste("p",0:length(k)-1, sep = "")))


ggplot(wp.tidy, aes(x = group, fill = group))+
  geom_bar() + 
  facet_wrap(~participant)
```


### Ex. 6.1.5 

What happens if you change the initial expectation that everybody is equally likely to belong to either group, and have an expectation that people generally are not guessing, with (say), group = flip(0.9)?


```{r 6.1.5,fig.width=10, fig.height=4, echo=FALSE}

```

# 6.2 Exam scores with individual differences 

The previous example shows how sampling can model data as coming from a mixture of sources, and infer properties of these latent groups. But the specific model has at least one big weakness, which is that it assumes all the people in the knowledge group have exactly the same rate of success on the questions.

One straightforward way to allow for individual differences in the knowledge group is to extend the model hierarchically. One convenient (but not perfect) choice for this “individual differences” distribution is a Gaussian. It has the problem of allowing for success rates below zero and above one. An inelegant but practical and effective way to deal with this is simply to restrict the sampled success rates to the valid range.

**Probably we should be doing something different than the LW Truncated gaussian**

```{r 6.2.1 ,fig.width=10, fig.height=4}

exam.scores.individualDiff <- '
// unpack data
var n_correct = observed_data["n_correct"];
var n_people = observed_data["n_people"];
var n_questions = observed_data["n_questions"];

var model = function() {
  
  var knowledgeSuccessRate = sample(UniformDrift({a: 0.5, b: 1, r:0.1}));
  var guessingSuccessRate = 0.5;
  var sigma = sample(UniformDrift({a: 0, b: 1, r:0.1}));

  foreach(_.range(0, n_people), function(id){

    var group =  flip(0.5) ? "knowledge" : "guessing";
    
    var individualKnowledgeG = gaussian(knowledgeSuccessRate, sigma)
    var individualKnowledge =  individualKnowledgeG > 0.999 ? 0.999 :
                         individualKnowledgeG < 0.001 ? 0.001 :
                         individualKnowledgeG 

    var successRate = group=="knowledge" ? individualKnowledge :
                        guessingSuccessRate

    observe({data: n_correct[id], 
              link: Binomial({n:n_questions, p:successRate})})
    
    query.add("p"+id, group)
    query.add("knowledge_"+id, successRate)

  })

  query.add("knowledgeRate", knowledgeSuccessRate)
  query.add("sigma", sigma)

  return query
}
'

k <- c(21,17,21,18,22,31,31,34,34,35,35,36,39,36,35)

observed_data = list(
  n_correct = k,
  n_questions = 40,
  n_people = length(k)
)

numSamples = 50000;

wp.samp <- webppl(program_code = exam.scores.individualDiff,
       data = observed_data,
       data_var = "observed_data",
       inference_opts = list(method="MCMC", 
                          samples = numSamples, 
                          burn = numSamples/2,
                          verbose = TRUE
                          ),
#        inference_opts = list(method="MCMC", 
#                              kernel = 
#                                list(HMC = 
#                                       list(steps = 5,
#                                            stepSize = 0.001)),
#                           samples = numSamples, 
#                           burn = numSamples/2,
#                           verbose = TRUE),
      model_var = "model",
      packages = c("./utils"),
      output_format = "samples"
      )

wp.samp %>% 
  select(starts_with("knowledge")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = c(paste("knowledge",0:14, sep = "_"), "knowledgeRate"))) %>%
  ggplot(., aes(x= value)) + 
  geom_histogram()+
  facet_wrap(~parameter)

wp.tidy <- wp.samp %>% 
  select(-starts_with("knowledge"), -sigma)  %>%
  gather(participant, group) %>%
  mutate(group = as.factor(group)) %>%
  mutate(participant = factor(participant,
                              levels = paste("p",0:14, sep = "")))

ggplot(wp.tidy, aes(x = group))+
  geom_bar() + 
  facet_wrap(~participant)
```

### Exercise 6.2.1 

Compare the results of the hierarchical model with the original model that did not allow for individual differences.

### Exercise 6.2.2 

Interpret the posterior distribution of the variable predphi. How does this distribution relate to the posterior distribution for mu?

```{r 6.2.2,fig.width=10, fig.height=4}
quartz('6.2.2 posterior of predphi')
qplot(data=melt(df$predphi),x=value,geom='histogram',binwidth=0.008)+
  theme_bw()+
  xlab('posterior predictive phi')+
  xlim(0,1)
```

### Exercise 6.2.3 

In what sense could the latent assignment of people to groups in this case study be considered a form of model selection?

# 6.3 Twenty Questions

Suppose a group of 10 people attend a lecture, and are asked a set of 20 questions afterwards, with every answer being either correct or incorrect. We want to infer two things: (1) how well each person attended to the lecture, (2) how hard each of the questions was.

One way to make these inferences is to specify a model of how a person’s attentiveness and a question’s difficulty combine to give an overall probability that the question will be answered correctly. A very simple model involves assuming that each person listens to some proportion of the lecture, and that each question has some probability of being answered correctly if the person was listening at the right point in the lecture.


```{r 6.3.1 ,fig.width=10, fig.height=8}
twentyQuestionsModel <- '
var data = [[1,1,1,1,0,0,1,1,0,1,0,0,1,0,0,1,0,1,0,0],
            [0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
            [0,0,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0],
            [0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0],
            [1,0,1,1,0,1,1,1,0,1,0,0,1,0,0,0,0,1,0,0],
            [1,1,0,1,0,0,0,1,0,1,0,1,1,0,0,1,0,1,0,0],
            [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],   
            [0,1,1,0,0,0,0,1,0,1,0,0,1,0,0,0,0,1,0,1],
            [1,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0]];
var n_people = data.length;
var n_questions = data[0].length;

var model = function() {
  var ps = repeat(n_people, function() {return beta(1,1)})
  var qs = repeat(n_questions, function() {return beta(1,1)})

  foreach(_.range(0, n_questions), function(question_id) {
    foreach(_.range(0, n_people), function(person_id) {
      var p_i = ps[person_id]
      var q_j = qs[question_id]
      var successRate = p_i * q_j
      observe({data: data[person_id][question_id] == 1, 
               link: Bernoulli({p: successRate})})
      query.add("successRate" + person_id + "_" + question_id, successRate)
    })
  })
  foreach(_.range(0, n_questions), function(i){query.add("question_" + i, qs[i])})
  foreach(_.range(0, n_people), function(i){query.add("person_" + i, ps[i])})
  return query
}
'

numSamples = 1000
wp.samp <- webppl(program_code = twentyQuestionsModel,
       inference_opts = list(method="MCMC",
                             kernel =
                               list(HMC =
                                      list(steps = 50,
                                           stepSize = 0.001)),
                          samples = numSamples,
                          burn = 50,
                          verbose = TRUE),
      model_var = "model",
      packages = c("./utils"),
      output_format = "samples"
      )

# Plot per-person params
my_a<- wp.samp %>%
  select(starts_with("person")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = paste("person",0:9, sep = "_"))) %>%
  ggplot(., aes(x= value)) + 
  geom_histogram(binwidth = 0.02)+
  facet_wrap(~parameter)

# Plot per-question params
my_b<-wp.samp %>%
  select(starts_with("question")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = c(paste("question",0:19, sep = "_"), "knowledgeRate"))) %>%
  ggplot(., aes(x= value)) + 
  geom_histogram(binwidth = 0.02)+
  facet_wrap(~parameter) +
  guides(fill = FALSE)
```

### Ex 6.3.1

Draw some conclusions about how well the various people listened, and about the difficulties of the various questions. Do the marginal posterior distributions you are basing your inference on seem intuitively reasonable?

### Ex 6.3.2 Missing data
```{r 6.3.2 ,fig.width=10, fig.height=8}

k <- c(1,1,1,1,0,0,1,1,0,1,0,0,NA,0,0,1,0,1,0,0,
        0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,0,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,
        0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,
        1,0,1,1,0,1,1,1,0,1,0,0,1,0,0,0,0,1,0,0,
        1,1,0,1,0,0,0,1,0,1,0,1,1,0,0,1,0,1,0,0,
        0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,
        0,0,0,0,NA,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,1,1,0,0,0,0,1,0,1,0,0,1,0,0,0,0,1,0,1,
        1,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,NA,0,0)

k <- matrix(k, nrow=10, byrow=T)

```

### 6.3.3 Rasch model

```{r 6.3.3 ,fig.width=10, fig.height=8}

```


# 6.4 The two-country quiz

Suppose a group of people take a historical quiz, and each answer for each person is scored as correct or incorrect. Some of the people are Thai, and some are Moldovan. Some of the questions are about Thai history, and it is more likely the answer would be known by a Thai person than a Moldovan. The rest of the questions are about Moldovan history, and it is more likely the answer would be known by a Moldovan than a Thai.

We do not know who is Thai or Moldovan, and we do not know the content of the questions.

A good way to make these inferences formally is to assume there are two types of answers. For those where the nationality of the person matches the origin of the question, the answer will be correct with high probability. For those where a person is being asked about the other country, the answer will have a very low probability of being correct.

```{r 6.4.1,fig.width=10, fig.height=8}
twoCountryModel <- '
var data = [[1,0,0,1,1,0,0,1],
            [1,0,0,1,1,0,0,1],
            [0,1,1,0,0,1,0,0],
            [0,1,1,0,0,1,1,0],
            [1,0,0,1,1,0,0,1],
            [0,0,0,1,1,0,0,1],
            [0,1,0,0,0,1,1,0],
            [0,1,1,1,0,1,1,0]]
var n_people = data.length;
var n_questions = data[0].length;

var model = function() {
  var alpha = uniform(0,1)
  var beta = uniform(0,alpha)
  var personAssignments = repeat(n_people, flip)
  var questionAssignments = repeat(n_questions, flip)

  foreach(_.range(0, n_questions), function(question_id) {
    foreach(_.range(0, n_people), function(person_id) {
      var pAssignment = personAssignments[person_id]
      var qAssignment = questionAssignments[question_id]
      var successRate = (pAssignment == qAssignment ? alpha : beta)
      if(!Bernoulli({p:successRate}).score(data[person_id][question_id] == 1)) {
        console.log(alpha + ", " + beta)
      }
      observe({data: data[person_id][question_id] == 1, 
               link: Bernoulli({p: successRate})})
      query.add("successRate" + person_id + "_" + question_id, successRate)
    })
  })

  query.add("alpha", alpha)
  query.add("beta", beta)

  // Add to query table
  foreach(_.range(0, n_questions), function(i){
    query.add("question_" + i, questionAssignments[i])
  })
  foreach(_.range(0, n_people), function(i){
    query.add("person_" + i, personAssignments[i])
  })
  return query
}
'

numSamples = 5000
wp.samp <- webppl(program_code = twoCountryModel,
       inference_opts = list(method="MCMC",
                             kernel =
                               list(HMC =
                                      list(steps = 25,
                                           stepSize = 0.001)),
                          samples = numSamples,
                          burn = 50,
                          verbose = TRUE),
      model_var = "model",
      packages = c("./utils"),
      output_format = "samples"
      )

# Plot alpha/beta
wp.samp %>% 
  select(alpha, beta) %>% 
  gather(parameter, value) %>%
  ggplot(aes(x= value, fill = parameter)) + 
  geom_histogram() +
  theme_bw()

# Plot per-person params
wp.samp %>%
  select(starts_with("person")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = paste("person",0:7, sep = "_"))) %>%
  ggplot(., aes(x= value, fill = value)) + 
  geom_bar()+
  facet_wrap(~parameter) +
  theme_bw()

# Plot per-question params
wp.samp %>%
  select(starts_with("question")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = paste("question",0:19, sep = "_"))) %>%
  ggplot(., aes(x= value, fill = value)) + 
  geom_bar()+
  facet_wrap(~parameter) +
  guides(fill = FALSE)

```


### Exercise 6.4.1 

Interpret the posterior distributions for x[i], z[j], alpha, and beta. Do the formal inferences agree with your original intuitions?

### Exercise 6.4.2 

The priors on the probabilities of answering correctly capture knowledge about what it means to match and mismatch, by imposing an order constraint α ≥ β. Change the code so that this information is not included, by using priors alpha∼dbeta(1,1) and beta∼dbeta(1,1). Run a few chains against the same data, until you get an inappropriate, and perhaps counter-intuitive, result. The problem that is being encountered is known as model indeterminacy or label-switching. Describe the problem, and discuss why it comes about.

```{r 6.4.2,fig.width=10, fig.height=8}

```

The problem is that the alpha (the probability of answering correctly for a question about your home country) becomes (in some chains) lower than beta (the probability of answering correctly for a question about a home country different from your own). This is because in this model formulation, alpha and beta are interchangable. 

#### Exercise 6.4.3 

Now suppose that three extra people enter the room late, and begin to take the quiz. One of them (Late Person 1) has answered the first four questions, the next (Late Person 2) has only answered the first question, and the final new person (Late Person 3) is still sharpening their pencil, and has not started the quiz. This situation can be represented as an updated data set, now with missing data. Interpret the inferences the model makes about the nationality of the late people, and whether or not they will get the unfinished questions correct.

```{r 6.4.3,fig.width=10, fig.height=16}

```



### Exercise 6.4.4 

Finally, suppose that you are now given the correctness scores for a set of 10 new people, whose data were not previously available, but who form part of the same group of people we are studying. Interpret the inferences the model makes about the nationality of the new people. Revisit the inferences about the late people, and whether or not they will get the unfinished questions correct. Does the inference drawn by the model for the third late person match your intuition? There is a problem here. How could it be fixed?

```{r 6.4.4,fig.width=10, fig.height=16}

```


