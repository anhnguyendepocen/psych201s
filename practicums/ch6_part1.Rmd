---
title: "ch6"
author: "mht"
date: "October 21, 2014"
output: html_document
---

# Setup

```{r}
library(rwebppl)
library(ggplot2)
library(tidyr)
library(dplyr)
library(jsonlite)

setwd("~/Repos/psych201s/practicums")

to.n <- function(n){return(as.numeric(as.character(n)))}

runModelMCMC <- function(model, data_to_webppl, 
                         numSamples = 50000) {
  wp <- webppl(
    program_code = model,
    data = data_to_webppl,
    data_var = "observed_data", 
       inference_opts = list(method="MCMC", 
                            samples = numSamples, 
                            burn = numSamples/2,
                            verbose = TRUE),
    model_var = "model",
    output_format = "samples",
    packages = c("./utils")
    )
}

runModelHMC <- function(model, data_to_webppl, 
                         numSamples = 50000, stepSize = 0.05) {
  wp <- webppl(
    program_code = model,
    data = data_to_webppl,
    data_var = "observed_data",
    inference_opts = list(method="MCMC", 
                             kernel = list(HMC = 
                                      list(steps = 10,
                                           stepSize = stepSize)),
                          samples = numSamples, 
                          burn = numSamples/2,
                          verbose = TRUE),
    model_var = "model",
    output_format = "samples",
    packages = c("./utils")
  )
}
```

# Chapter 6: Latent Mixture Models

Suppose a group of 15 people sit an exam made up of 40 true-or-false questions, and they get 21, 17, 21, 18, 22, 31, 31, 34, 34, 35, 35, 36, 39, 36, and 35 right. These scores suggest that the first 5 people were just guessing, but the last 10 had some level of knowledge.

One way to make statistical inferences along these lines is to assume there are two different groups of people. These groups have different probabilities of success, with the guessing group having a probability of 0.5, and the knowledge group having a probability greater than 0.5. Whether each person belongs to the first or the second group is a latent or unobserved variable that can take just two values. Using this approach, the goal is to infer to which group each person belongs, and also the rate of success for the knowledge group.

This type of model is known as a latent-mixture model, because the data are assumed to be generated by two different processes that combine or mix, and im- portant properties of that mixture are unobserved or latent. In this case, the two components that mix are the guessing and knowledge processes, and the group membership of each person is latent.

Recall that in WebPPL, `uniform(0.5,1)` is the same as `sample(Uniform({a:0.5, b:1}))`. Also, `flip(0.5)` is shorthand for `bernoulli(0.5)`.

```{r 6.1.1,fig.width=10, fig.height=4}
exam.scores <- '
// unpack data
var n_correct = observed_data["n_correct"];
var n_people = observed_data["n_people"];
var n_questions = observed_data["n_questions"];

var model = function() {
  
  var knowledgeSuccessRate = uniform(0.5, 1)
  var guessingSuccessRate = 0.5  

  foreach(_.range(0, n_people), function(id){

    var group =  flip(0.5) ? "knowledge" : "guessing";
    var successRate = (group=="knowledge" ? 
                       knowledgeSuccessRate : 
                       guessingSuccessRate);

    observe({
      data: n_correct[id], 
      link: Binomial({n:n_questions, p:successRate})
    })
    
    query.add("p" + id, group)
  })

  query.add("knowledgeRate", knowledgeSuccessRate)

  return query
}
'

k <- c(21,17,21,18,22,31,31,34,34,35,35,36,39,36,35)

observed_data = list(
  n_correct = k,
  n_questions = 40,
  n_people = length(k)
)

numSamples = 50000;

wp.samp <- runModelMCMC(exam.scores, observed_data, numSamples)

# As always, first look at what your output looks like:

head(wp.samp)

# plot the inferred "knowledge rate"
ggplot(wp.samp, aes(x= knowledgeRate)) + 
  geom_histogram(binwidth = 0.01)+
  xlim(0.5,1)

# plot posterior on group variable for each participant
wp.samp %>% 
  gather(participant, group, -knowledgeRate) %>%
  mutate(participant = factor(participant,
                              levels = paste("p",0:length(k)-1, sep = ""))) %>%
  ggplot(aes(x = group))+
    geom_bar() + 
    facet_wrap(~participant)
```

### Ex 6.1.1

Draw some conclusions about the problem from the posterior distribution.
Who belongs to what group, and how confident are you?

### SKIP Ex 6.1.2

### Ex 6.1.3 

Include an extra person in the exam, with a score of 28 out of 40. What does their posterior for group tell you? 

...

Now add four extra people, all with the score 28 out of 40. Explain the change these extra people make to the inference.

...

```{r 6.1.3a,fig.width=10, fig.height=4, echo=FALSE}
# ADD NEW SCORES HERE
k <- c(21,17,21,18,22,31,31,34,34,35,35,36,39,36,35,
       ... )

observed_data = list(
  n_correct = k,
  n_questions = 40,
  n_people = length(k)
)

wp.samp <- runModelMCMC(exam.scores, observed_data, numSamples)

# plot the inferred "knowledge rate"
ggplot(wp.samp, aes(x= knowledgeRate)) + 
  geom_histogram(binwidth = 0.01)+
  xlim(0.5,1)

# plot group membership posteriors
wp.samp %>% 
  select(-knowledgeRate) %>%
  gather(participant, group) %>%
  mutate(participant = factor(participant,
                              levels = paste("p",0:length(k)-1, sep = ""))) %>%
  ggplot(aes(x = group, fill = group))+
    geom_bar() + 
    facet_wrap(~participant)
```


### Ex. 6.1.4 

What happens if you change the prior on the success rate of the second group to be uniform over the whole range from 0 to 1, and so allow for worse-than-guessing performance?


```{r 6.1.4,fig.width=10, fig.height=4}
exam.scores2 <- '
// unpack data
var n_correct = observed_data["n_correct"];
var n_people = observed_data["n_people"];
var n_questions = observed_data["n_questions"];


var model = function() {
  
  var knowledgeSuccessRate = uniform(0, 1)
  var guessingSuccessRate = 0.5  

  foreach(_.range(0, n_people), function(id){

    var group =  flip(0.5) ? "knowledge" : "guessing";
    var successRate = group=="knowledge" ? 
                        knowledgeSuccessRate : 
                        guessingSuccessRate

    observe({data: n_correct[id], 
              link: Binomial({n:n_questions, p:successRate})})
    
    query.add("p" + id, group)

  })

  query.add("knowledgeRate", knowledgeSuccessRate)
  query.add("guessingRate", guessingSuccessRate)

  return query
}
'

k <- c(21,17,21,18,22,31,31,34,34,35,35,36,39,36,35)

observed_data = list(
  n_correct = k,
  n_questions = 40,
  n_people = length(k)
)

numSamples = 50000;

wp.samp <- runModelMCMC(exam.scores2, observed_data, numSamples)

wp.samp %>% select(knowledgeRate, guessingRate) %>%
  gather(parameter, value) %>%
  ggplot(aes(x= value)) + 
  geom_histogram()+
  facet_wrap(~parameter)

wp.tidy <- wp.samp %>% 
  select(-knowledgeRate, -guessingRate) %>%
  gather(participant, group) %>%
  mutate(participant = factor(participant,
                              levels = paste("p",0:length(k)-1, sep = "")))


ggplot(wp.tidy, aes(x = group, fill = group))+
  geom_bar() + 
  facet_wrap(~participant)
```


### Ex. 6.1.5 

What happens if you change the initial expectation that everybody is equally likely to belong to either group, and have an expectation that people generally are not guessing, with (say), group = flip(0.9)?


```{r 6.1.5,fig.width=10, fig.height=4, echo=FALSE}

```

# 6.2 Exam scores with individual differences 

The previous example shows how sampling can model data as coming from a mixture of sources, and infer properties of these latent groups. But the specific model has at least one big weakness, which is that it assumes all the people in the knowledge group have exactly the same rate of success on the questions.

One straightforward way to allow for individual differences in the knowledge group is to extend the model hierarchically. One convenient (but not perfect) choice for this “individual differences” distribution is a Gaussian. It has the problem of allowing for success rates below zero and above one. An inelegant but practical and effective way to deal with this is simply to restrict the sampled success rates to the valid range.

```{r 6.2.1 ,fig.width=10, fig.height=4}

exam.scores.individualDiff <- '
// unpack data
var n_correct = observed_data["n_correct"];
var n_people = observed_data["n_people"];
var n_questions = observed_data["n_questions"];

var truncatedGaussian = function(mu, sigma){
  var s = gaussian(mu, sigma);
  factor(s < 0 || s > 1 ? -Infinity : 0);
  return s;
}

var model = function() {
  
  var knowledgeSuccessRate = sample(UniformDrift({a: 0.5, b: 1, r:0.1}));
  var guessingSuccessRate = 0.5;
  var sigma = sample(UniformDrift({a: 0, b: 1, r:0.1}));

  foreach(_.range(0, n_people), function(id){

    var group =  flip(0.5) ? "knowledge" : "guessing";
    
    var individualKnowledge = truncatedGaussian(knowledgeSuccessRate, sigma)

    var successRate = (group=="knowledge" ? 
                        individualKnowledge :
                        guessingSuccessRate)

    observe({data: n_correct[id], 
              link: Binomial({n:n_questions, p:successRate})})
    
    query.add("p"+id, group)
    query.add("knowledge_"+id, successRate)

  })

  query.add("knowledgeRate", knowledgeSuccessRate)
  query.add("sigma", sigma)
  query.add("predictiveRate", truncatedGaussian(knowledgeSuccessRate, sigma))
 
  return query
}
'

k <- c(21,17,21,18,22,31,31,34,34,35,35,36,39,36,35)

observed_data = list(
  n_correct = k,
  n_questions = 40,
  n_people = length(k)
)

numSamples = 100000;

wp.samp <- runModelMCMC(exam.scores.individualDiff,observed_data, numSamples)

# Plot individual knowledge rates
wp.samp %>% 
  select(starts_with("knowledge")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = c(paste("knowledge",0:14, sep = "_"),
                                       "knowledgeRate"))) %>%
  ggplot(., aes(x= value)) + 
  geom_histogram()+
  facet_wrap(~parameter)

# Plot membership beliefs
wp.samp %>% 
  select(-starts_with("knowledge"), -sigma, -predictiveRate)  %>%
  gather(participant, group) %>%
  mutate(group = as.factor(group)) %>%
  mutate(participant = factor(participant,
                              levels = paste("p",0:14, sep = ""))) %>%
  ggplot(aes(x = group))+
    geom_bar() + 
    facet_wrap(~participant)
```

### Exercise 6.2.1 

Compare the results of the hierarchical model with the original model that did not allow for individual differences.

### Exercise 6.2.2 

Interpret the posterior distribution of the variable predictiveRate. How does this distribution relate to the posterior distribution for knowledgeRate?

```{r 6.2.2,fig.width=10, fig.height=4}

wp.samp %>% 
  select(knowledgeRate, predictiveRate) %>%
  gather(parameter, value) %>%
  ggplot(., aes(x = value))+
  geom_histogram()+
  facet_wrap(~parameter)

```

### Exercise 6.2.3 

In what sense could the latent assignment of people to groups in this case study be considered a form of model selection?

# 6.3 Twenty Questions

Suppose a group of 10 people attend a lecture, and are asked a set of 20 questions afterwards, with every answer being either correct or incorrect. We want to infer two things: (1) how well each person attended to the lecture, (2) how hard each of the questions was.

One way to make these inferences is to specify a model of how a person’s attentiveness and a question’s difficulty combine to give an overall probability that the question will be answered correctly. A very simple model involves assuming that each person listens to some proportion of the lecture, and that each question has some probability of being answered correctly if the person was listening at the right point in the lecture.


```{r 6.3.1 ,fig.width=10, fig.height=8}
twentyQuestionsModel <- '
var people = utils.uniq(utils.pluck(observed_data, "participant"));
var questions = utils.uniq(utils.pluck(observed_data, "question"))
var n_people = people.length;
var n_questions = questions.length;

var model = function() {
  var ps = repeat(n_people, function() {return beta(1,1)})
  var qs = repeat(n_questions, function() {return beta(1,1)})

  foreach(questions, function(question_id) {
    var questionData = subset(observed_data, "question", question_id);

    foreach(people, function(person_id) {
      var personQuestionData = subset(questionData, "participant", person_id);

      var p_i = ps[person_id-1]
      var q_j = qs[question_id-1]

      var successRate = p_i * q_j;

      observe({
          data: utils.pluck(personQuestionData,"correctResponse"), 
          link: Bernoulli({p: successRate})
      })

      query.add("successRate" + person_id + "_" + question_id, successRate)

    })

  })
  foreach(utils.range(0, n_questions), 
            function(i){query.add("question_" + i, qs[i])})
  foreach(utils.range(0, n_people), 
            function(i){query.add("person_" + i, ps[i])})
  return query
}
'

# format data

observed_data <- data.frame(
fromJSON("
          [
          [1,1,1,1,0,0,1,1,0,1,0,0,1,0,0,1,0,1,0,0],
          [0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
          [0,0,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0],
          [0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0],
          [1,0,1,1,0,1,1,1,0,1,0,0,1,0,0,0,0,1,0,0],
          [1,1,0,1,0,0,0,1,0,1,0,1,1,0,0,1,0,1,0,0],
          [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],
          [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],   
          [0,1,1,0,0,0,0,1,0,1,0,0,1,0,0,0,0,1,0,1],
          [1,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0]
          ]
         ")) %>% 
  mutate(person = row_number()) %>%
  gather(question, response, -person) %>% 
  mutate(question = gsub("X", "", question)) %>%
  mutate(participant = to.n(person),
         question = to.n(question),
         response = response==1) %>%
  select(-person) %>%
  spread(question, response)


print(observed_data)

# make tidy data
data_tidy <- observed_data %>%
  gather(question, correctResponse, -participant)

# look at data again
head(data_tidy)

numSamples = 2000
wp.samp <- runModelHMC(twentyQuestionsModel, data_tidy, numSamples)


```



```{r}
# Plot per-person params
wp.samp %>%
  select(starts_with("person")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = paste("person",0:9, sep = "_"))) %>%
  ggplot(., aes(x= value)) + 
  geom_histogram(binwidth = 0.02)+
  facet_wrap(~parameter)



# Plot per-question params
wp.samp %>%
  select(starts_with("question")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = c(paste("question",0:19, sep = "_"),
                                       "knowledgeRate"))) %>%
  ggplot(., aes(x= value)) + 
  geom_histogram(binwidth = 0.02)+
  facet_wrap(~parameter) +
  guides(fill = FALSE)
```

### Ex 6.3.1

Draw some conclusions about how well the various people listened, and about the difficulties of the various questions. Do the marginal posterior distributions you are basing your inference on seem intuitively reasonable?

### Ex 6.3.2 Missing data

Now suppose that three of the answers were not recorded, for what- ever reason. Our new data set, with missing data, now takes the form shown in Table 6.2. Bayesian inference will automatically make predictions about these missing values (i.e., “fill in the blanks”) by using the same probabilistic model that generated the observed data. Missing data are entered NA (“not available”). Including the variable k as one to monitor when sampling will then provide posterior values for the missing values. That is, it provides information about the relative likelihood of the missing values being each of the possible alter- natives, using the statistical model and the available data. Look through the R code to see how all of this is implemented in the second data set. Run the code, and interpret the posterior distributions for the three missing values. Are they reasonable inferences?

```{r 6.3.2 ,fig.width=10, fig.height=8}

k <- c(1,1,1,1,0,0,1,1,0,1,0,0,NA,0,0,1,0,1,0,0,
        0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,0,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,
        0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,
        1,0,1,1,0,1,1,1,0,1,0,0,1,0,0,0,0,1,0,0,
        1,1,0,1,0,0,0,1,0,1,0,1,1,0,0,1,0,1,0,0,
        0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,
        0,0,0,0,NA,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,1,1,0,0,0,0,1,0,1,0,0,1,0,0,0,0,1,0,1,
        1,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,NA,0,0)

k <- matrix(k, nrow=10, byrow=T)

observed_data <- data.frame(k) %>% 
  mutate(person = row.names(observed_data)) %>%
  gather(question, response, -person) %>% 
  mutate(question = gsub("X", "", question)) %>%
  mutate(participant = to.n(person),
         question = to.n(question),
         response = response==1) %>%
  select(-person) %>%
  spread(question, response)

data_tidy <- observed_data %>%
  gather(question, correctResponse, -participant)

# remove missing data from data frame
data_tidy <- data_tidy[complete.cases(data_tidy),]

# look at data again
head(data_tidy)

numSamples = 2000
wp.samp <-runModelHMC(twentyQuestionsModel, data_tidy, numSamples)

# Note first number is Person #, second is Question #
wp.samp %>%
  select(starts_with("successRate")) %>% 
  gather(key, val) %>% 
  filter(key %in% c("successRate1_13", "successRate8_5", "successRate10_18")) %>%
  ggplot(., aes(x = val))+
  geom_histogram()+
  facet_wrap(~key)
```

### 6.3.3 Rasch model

The definition of the accuracy for a person on a question in terms of the product theta ij = pi*qj is very simple to understand, but other models of the interaction between person ability and question difficulty are used in psychometric models. For example, the Rasch model (e.g., Andrich, 1988) uses theta ij = exp(pi - qj)/(1 + exp(pi − qj)). Change the graphical model to implement the Rasch model.

Copy over the code from above, find where we compute theta, and plug in this new accuracy function.

```{r 6.3.3 ,fig.width=10, fig.height=8}
raschModel <- '
var oldAccuracyFunction = function(p_i, q_j) {
  return p_i * q_j;
};

var newAccuracyFunction = function(p_i, q_j) {
  return Math.exp(pi - qj)/(1 + Math.exp(pi - qj));
}

...

'
```


# 6.4 The two-country quiz

Suppose a group of people take a historical quiz, and each answer for each person is scored as correct or incorrect. Some of the people are Thai, and some are Moldovan. Some of the questions are about Thai history, and it is more likely the answer would be known by a Thai person than a Moldovan. The rest of the questions are about Moldovan history, and it is more likely the answer would be known by a Moldovan than a Thai.

We do not know who is Thai or Moldovan, and we do not know the content of the questions.

A good way to make these inferences formally is to assume there are two types of answers. For those where the nationality of the person matches the origin of the question, the answer will be correct with high probability. For those where a person is being asked about the other country, the answer will have a very low probability of being correct.

```{r 6.4.1,fig.width=10, fig.height=8}
twoCountryModel <- '
var people = utils.uniq(utils.pluck(observed_data, "participant"));
var questions = utils.uniq(utils.pluck(observed_data, "question"))
var n_people = people.length;
var n_questions = questions.length;

var model = function() {
  var alpha = uniform(0,1)
  var beta = uniform(0,alpha)
  var personAssignments = repeat(n_people, flip)
  var questionAssignments = repeat(n_questions, flip)

  foreach(questions, function(question_id) {
    var questionData = subset(observed_data, "question", question_id)

    foreach(people, function(person_id) {
      var personQuestionData = subset(questionData, "participant", person_id)

      var pAssignment = personAssignments[person_id - 1]
      var qAssignment = questionAssignments[question_id - 1]

      var successRate = (pAssignment == qAssignment ? alpha : beta)
      observe({
        data: utils.pluck(personQuestionData, "correctResponse"),
        link: Bernoulli({p: successRate})
      })
      query.add("successRate" + person_id + "_" + question_id, successRate)
    })
  })

  query.add("alpha", alpha)
  query.add("beta", beta)

  // Add to query table
  foreach(utils.range(0, n_questions), function(i){
    query.add("question_" + i, questionAssignments[i])
  })
  foreach(utils.range(0, n_people), function(i){
    query.add("person_" + i, personAssignments[i])
  })
  return query
}
'

observed_data <- data.frame(
  fromJSON("[[1,0,0,1,1,0,0,1],
             [1,0,0,1,1,0,0,1],
             [0,1,1,0,0,1,0,0],
             [0,1,1,0,0,1,1,0],
             [1,0,0,1,1,0,0,1],
             [0,0,0,1,1,0,0,1],
             [0,1,0,0,0,1,1,0],
             [0,1,1,1,0,1,1,0]]")) %>%
  mutate(person = row_number()) %>%
  gather(question, response, -person) %>% 
  mutate(question = gsub("X", "", question)) %>%
  mutate(participant = to.n(person),
         question = to.n(question),
         response = response==1) %>%
  select(-person) %>%
  spread(question, response)

# make tidy data
data_tidy <- observed_data %>%
  gather(question, correctResponse, -participant)

print(data_tidy)
numSamples = 1000
#wp.samp <- runModelMCMC(twoCountryModel, data_tidy, numSamples)
wp.samp <- webppl(program_code = twoCountryModel,
        data_var = "observed_data",
        data = data_tidy,
       inference_opts = list(method="MCMC",
                             kernel =
                               list(HMC =
                                      list(steps = 25,
                                           stepSize = 0.001)),
                          samples = numSamples,
                          burn = numSamples/2,
                          verbose = TRUE),
      model_var = "model",
      packages = c("./utils"),
      output_format = "samples"
      )
```

```{r}

# Plot alpha/beta
wp.samp %>% 
  select(alpha, beta) %>% 
  gather(parameter, value) %>%
  ggplot(aes(x= value, fill = parameter)) + 
  geom_histogram() +
  theme_bw()

# Plot per-person params
wp.samp %>%
  select(starts_with("person")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = paste("person",0:7, sep = "_"))) %>%
  ggplot(., aes(x= value, fill = value)) + 
  geom_bar()+
  facet_wrap(~parameter) +
  theme_bw()

# Plot per-question params
wp.samp %>%
  select(starts_with("question")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = paste("question",0:19, sep = "_"))) %>%
  ggplot(., aes(x= value, fill = value)) + 
  geom_bar()+
  facet_wrap(~parameter) +
  guides(fill = FALSE)

```


### Exercise 6.4.1 

Interpret the posterior distributions for x[i], z[j], alpha, and beta. Do the formal inferences agree with your original intuitions?

### Exercise 6.4.2 

The priors on the probabilities of answering correctly capture knowledge about what it means to match and mismatch, by imposing an order constraint alpha >= beta. Change the code so that this information is not included, by using priors alpha∼beta(1,1) and beta∼beta(1,1). Run a few chains against the same data, until you get an inappropriate, and perhaps counter-intuitive, result. The problem that is being encountered is known as model indeterminacy or label-switching. Describe the problem, and discuss why it comes about.

```{r 6.4.2,fig.width=10, fig.height=8}

```

The problem is that the alpha (the probability of answering correctly for a question about your home country) becomes (in some chains) lower than beta (the probability of answering correctly for a question about a home country different from your own). This is because in this model formulation, alpha and beta are interchangable. 

#### Exercise 6.4.3 

Now suppose that three extra people enter the room late, and begin to take the quiz. One of them (Late Person 1) has answered the first four questions, the next (Late Person 2) has only answered the first question, and the final new person (Late Person 3) is still sharpening their pencil, and has not started the quiz. This situation can be represented as an updated data set, now with missing data. Interpret the inferences the model makes about the nationality of the late people, and whether or not they will get the unfinished questions correct.

```{r 6.4.3,fig.width=10, fig.height=16}
k <- c(1,0,0,1,1,0,0,1,
       1,0,0,1,1,0,0,1,
       0,1,1,0,0,1,0,0,
       0,1,1,0,0,1,1,0,
       1,0,0,1,1,0,0,1,
       0,0,0,1,1,0,0,1,
       0,1,0,0,0,1,1,0,
       0,1,1,1,0,1,1,0,
       1,0,0,1,NA,NA,NA,NA,
       0,NA,NA,NA,NA,NA,NA,NA,
       NA,NA,NA,NA,NA,NA,NA,NA);

k <- matrix(k, nrow=11, byrow=T)

observed_data <- data.frame(k) %>%
  mutate(person = row_number()) %>%
  gather(question, response, -person) %>% 
  mutate(question = gsub("X", "", question)) %>%
  mutate(participant = to.n(person),
         question = to.n(question),
         response = response==1) %>%
  select(-person) %>%
  spread(question, response)

# make tidy data
data_tidy <- observed_data %>%
  gather(question, correctResponse, -participant)

data_tidy <- data_tidy[complete.cases(data_tidy),]

numSamples = 1000
#wp.samp <- runModelMCMC(twoCountryModel, data_tidy, numSamples)
wp.samp <- webppl(program_code = twoCountryModel,
        data_var = "observed_data",
        data = data_tidy,
       inference_opts = list(method="MCMC",
                             kernel =
                               list(HMC =
                                      list(steps = 25,
                                           stepSize = 0.001)),
                          samples = numSamples,
                          burn = numSamples/2,
                          verbose = TRUE),
      model_var = "model",
      packages = c("./utils"),
      output_format = "samples"
      )

# WRITE YOUR OWN ANALYSIS CODE HERE; see 6.3.2 for hints
```



### Exercise 6.4.4 

Finally, suppose that you are now given the correctness scores for a set of 10 new people, whose data were not previously available, but who form part of the same group of people we are studying. Interpret the inferences the model makes about the nationality of the new people. Revisit the inferences about the late people, and whether or not they will get the unfinished questions correct. Does the inference drawn by the model for the third late person match your intuition? There is a problem here. How could it be fixed?

```{r 6.4.4,fig.width=10, fig.height=16}
# Note: the way we extract n_participants in our webppl model 
# just counts unique numbers in the participant column, 
# and will miss the fully missing row. This should be fixed either by 
# passing in num_participants explicitly, or using another data representation 

k <- c(rep(c(1,0,0,1,1,0,0,1), 10),
       1,0,0,1,1,0,0,1,
       1,0,0,1,1,0,0,1,
       0,1,1,0,0,1,0,0,
       0,1,1,0,0,1,1,0,
       1,0,0,1,1,0,0,1,
       0,0,0,1,1,0,0,1,
       0,1,0,0,0,1,1,0,
       0,1,1,1,0,1,1,0,
       1,0,0,1,NA,NA,NA,NA,
       0,NA,NA,NA,NA,NA,NA,NA,
       NA,NA,NA,NA,NA,NA,NA,NA);

k <- matrix(k, nrow=21, byrow=T)

observed_data <- data.frame(k) %>%
  mutate(person = row_number()) %>%
  gather(question, response, -person) %>% 
  mutate(question = gsub("X", "", question)) %>%
  mutate(participant = to.n(person),
         question = to.n(question),
         response = response==1) %>%
  select(-person) %>%
  spread(question, response)

# make tidy data
data_tidy <- observed_data %>%
  gather(question, correctResponse, -participant)

data_tidy <- data_tidy[complete.cases(data_tidy),]
```


