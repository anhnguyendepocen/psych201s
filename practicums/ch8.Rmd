---
title: "ch8"
author: "mht"
date: "October 30, 2014"
output: html_document
---

# Chapter 8: Comparing Gaussian Means

When we use the one-sample t-test, we assume that the data follow a Gaussian distribution with unknown mean μ and unknown variance σ2. This is a natural assumption for a within-subjects experimental design, like that undertaken by Dr Smith. The data consist of one sample of standardized difference scores (i.e., “winter scores − summer scores”). The null hypothesis states that the mean of the difference scores is equal to zero, that is, H0 : μ = 0. The alternative hypothesis states that themeanisnotequaltozero,thatis,H1 :μ̸=0.

## 8.1 One-sample comparison

```{r libraries, fig.width=10, fig.height=4, echo=FALSE}
library(rwebppl)
library(dplyr)
library(tidyr)
library(ggplot2)
library(polspline)


getwd()
setwd("~/Repos/psych201s/practicums/")

```


```{r 8.1.1}
model<- 
'// One-Sample Comparison of Means
var model = function(){
  var delta = sample(Cauchy({location: 0, scale: 1}));
  var sigma = Math.abs(sample(Cauchy({location: 0, scale: 1})));
  var mu = delta*sigma;
  observe({
    data: data,
    link: Gaussian({mu:mu, sigma: sigma})
  })
  return {
    delta: delta,
    sigma: sigma
  }
}'



# Read data Dr. Smith
Winter <- c(-0.05,0.41,0.17,-0.13,0.00,-0.05,0.00,0.17,0.29,0.04,0.21,0.08,0.37,0.17,0.08,-0.04,-0.04,0.04,-0.13,-0.12,0.04,0.21,0.17,
       0.17,0.17,0.33,0.04,0.04,0.04,0.00,0.21,0.13,0.25,-0.05,0.29,0.42,-0.05,0.12,0.04,0.25,0.12)
 
Summer <- c(0.00,0.38,-0.12,0.12,0.25,0.12,0.13,0.37,0.00,0.50,0.00,0.00,-0.13,-0.37,-0.25,-0.12,0.50,0.25,0.13,0.25,0.25,0.38,0.25,0.12,
      0.00,0.00,0.00,0.00,0.25,0.13,-0.25,-0.38,-0.13,-0.25,0.00,0.00,-0.12,0.25,0.00,0.50,0.00)

x <- Winter-Summer # allowed because it is a within-subjects design
x <- x/sd(x)       # standardize

numSamples = 10000
rs <- webppl(model,
       model_var= "model",
       inference_opts = list(method = "MCMC",
                             samples = numSamples,
                             burn = numSamples/2,
                             verbose = TRUE),
      packages = c("./utils"),
      output_format = "samples",
      data = x,
      data_var = "data"
)


rs.tidy <- rs %>% 
  gather(param, value)

ggplot(rs.tidy, aes(x = value))+
  geom_histogram()+
  facet_wrap(~param, scales = 'free')
```

Compute Bayes Factor using the Savage-Dickey method

```{r}
fit.posterior <- logspline(rs$delta)

# posterior density at critical value: delta = 0
posteriorProbability <- dlogspline(0, fit.posterior)

# prior density at critical value: delta = 0
priorProbability <- dcauchy(x=0, location = 0, scale = 1)

# bayes factor 01: null vs. alternative
BF01 <- posteriorProbability/priorProbability

print(paste("The null hypothesis is", BF01, "times more likely than the alternative hypothesis."))
```

The Bayes factor between H0 and H1 is `r BF01`

### Exercise 8.1.1 

Here we assumed a half-Cauchy prior distribution on the standard deviation sigma. Other choices are possible and reasonable. Can you think of a few?

### Exercise 8.1.2 

Do you think the different priors on sigma will lead to substantially different conclusions? Why or why not? Convince yourself by implementing a different prior and studying the result.


### Exercise 8.1.3 

We also assumed a Cauchy prior distribution on effect size delta. Other choices are possible and reasonable. One such choice is the standard Gaussian distribution. Do you think this prior will lead to substantially different conclusions? Why or why not? Convince yourself by implementing the standard Gaussian prior and studying the result.


## 8.2 Order-restricted one-sample comparison

Order-restricted hypothesis is also known as one-sided hypothesis.
```{r 8.2}
model<- 
'
var model = function(){
  var delta = -1*Math.abs(sample(Cauchy({location: 0, scale: 1})));
  var sigma = Math.abs(sample(Cauchy({location: 0, scale: 1})));
  var mu = delta*sigma;
  observe({
    data: data,
    link: Gaussian({mu:mu, sigma: sigma})
  })
  return {
    delta: delta,
    sigma: sigma
  }
}'



# Read data Dr. Smith
Winter <- c(-0.05,0.41,0.17,-0.13,0.00,-0.05,0.00,0.17,0.29,0.04,0.21,0.08,0.37,0.17,0.08,-0.04,-0.04,0.04,-0.13,-0.12,0.04,0.21,0.17,
       0.17,0.17,0.33,0.04,0.04,0.04,0.00,0.21,0.13,0.25,-0.05,0.29,0.42,-0.05,0.12,0.04,0.25,0.12)
 
Summer <- c(0.00,0.38,-0.12,0.12,0.25,0.12,0.13,0.37,0.00,0.50,0.00,0.00,-0.13,-0.37,-0.25,-0.12,0.50,0.25,0.13,0.25,0.25,0.38,0.25,0.12,
      0.00,0.00,0.00,0.00,0.25,0.13,-0.25,-0.38,-0.13,-0.25,0.00,0.00,-0.12,0.25,0.00,0.50,0.00)

x <- Winter-Summer # allowed because it is a within-subjects design
x <- x/sd(x)       # standardize

numSamples = 10000
rs <- webppl(model,
       model_var= "model",
       inference_opts = list(method = "MCMC",
                             samples = numSamples,
                             burn = numSamples/2,
                             verbose = TRUE),
      packages = c("./utils"),
      output_format = "samples",
      data = x,
      data_var = "data"
)


rs.tidy <- rs %>% 
  gather(param, value)

ggplot(rs.tidy, aes(x = value))+
  geom_histogram()+
  facet_wrap(~param, scales = 'free')
```

Compute Bayes Factor using the Savage-Dickey method

```{r}
fit.posterior <- logspline(rs$delta)

# posterior density at critical value: delta = 0
posteriorProbability <- dlogspline(0, fit.posterior)

# prior density at critical value: delta = 0
priorProbability <- dcauchy(x=0, location = 0, scale = 1)

# bayes factor 01: null vs. alternative
BF01 <- posteriorProbability/priorProbability

print(paste("The null hypothesis is", BF01, "times more likely than the alternative hypothesis."))
```

### Exercise 8.2.1 

For completeness, estimate the Bayes factor for the summer and winter data between H0 : δ = 0 versus H3 : Cauchy􏰈0, 1􏰉I(0,∞,), involving the order-restricted alternative hypothesis that assumes the effect is positive.

### Exercise 8.2.2

In this example, it matters whether the alternative hypothesis is unrestricted, order-restricted to negative values for δ, or order-restricted to positive values for δ. Why is this perfectly reasonable? Can you think of a situation where the three versions of the alternative hypothesis yield exactly the same Bayes factor?

### Exercise 8.2.3  

From a practical standpoint, we do not need a new graphical model and WebPPL script to compute the Bayes factor for H0 versus the order- restricted H2. Instead, we can use the original graphical model in Figure 8.1 that implements the unrestricted Cauchy distribution and discard those prior and posterior MCMC samples that are inconsistent with the δ < 0 order- restriction. The Savage–Dicky density ratio test still involves the height of the prior and posterior distributions at δ = 0, but now the samples from these distributions are truncated, respecting the order-restriction, such that they range only from δ = −∞ to δ = 0. 

# 8.3 Two sample

```{r 8.3,fig.width=10, fig.height=4, echo=FALSE}
twoSampleModel <- 
'// One-Sample Comparison of Means
var model = function(){
  // standardized effect size
  var delta = sample(Cauchy({location: 0, scale: 1}));

  // mean of means
  var mu = sample(Cauchy({location: 0, scale: 1}));

  // standard deviation
  var sigma = Math.abs(sample(Cauchy({location: 0, scale: 1})));

  // difference between the means
  var alpha = delta*sigma;

  observe({
    data: data.x,
    link: Gaussian({mu: mu + alpha/2, sigma: sigma})
  })

  observe({
    data: data.y,
    link: Gaussian({mu: mu - alpha/2, sigma: sigma})
  })

  return {
    delta: delta,
    mu: mu,
    sigma: sigma
  }
}'
# Read data Dr. Smith
x <- c(70,80,79,83,77,75,84,78,75,75,78,82,74,81,72,70,75,72,76,77)

y <- c(56,80,63,62,67,71,68,76,79,67,76,74,67,70,62,65,72,72,69,71)

# Rescale
y <- (y - mean(x))/sd(x)
x <- (x - mean(x))/sd(x)

observed_data = list(x = x, y = y)
numSamples = 5000
rs <- webppl(twoSampleModel,
       model_var= "model",
       inference_opts = list(method = "MCMC",
                             kernel = list(
                               HMC = list(
                                 steps = 5,
                                 stepSize = 0.01
                               )
                             ),
                             samples = numSamples,
                             burn = numSamples/2,
                             verbose = TRUE),
      packages = c("./utils"),
      output_format = "samples",
      data = observed_data,
      data_var = "data"
)


rs.tidy <- rs %>% 
  gather(param, value)

ggplot(rs.tidy, aes(x = value))+
  geom_histogram()+
  facet_wrap(~param, scales = 'free')
```

```{r}
fit.posterior <- logspline(rs$delta)

# posterior density at critical value: delta = 0
posteriorProbability <- dlogspline(0, fit.posterior)

# prior density at critical value: delta = 0
priorProbability <- dcauchy(x=0, location = 0, scale = 1)

# bayes factor 01: null vs. alternative
BF01 <- posteriorProbability/priorProbability

print(paste("The null hypothesis is", BF01, "times more likely than the alternative hypothesis."))

print(paste("In order words, the alternative hypothesis is", 1/BF01, "times more likely than the null hypothesis."))

```

## Exercise 8.3.1 

The two-sample comparison of means outlined above assumes that the two groups have equal variance. How can you extend the model when this assumption is not reasonable?
