---
title: "linear-regression"
author: "mht"
date: "June 26, 2016"
output: html_document
---
```{r}
library(rwebppl)
library(ggplot2)
library(tidyr)
library(dplyr)
library(coda)

setwd("~/Repos/psych201s/practicums")

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

HPDhi<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

HPDlo<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}

runModelMCMC <- function(model, data_to_webppl, 
                         numSamples = 50000) {
  wp <- webppl(
    program_code = model,
    data = data_to_webppl,
    data_var = "observed_data", 
    inference_opts = list(method="MCMC", 
                          samples = numSamples, 
                          burn = numSamples/2,
                          verbose = TRUE),
    model_var = "model",
    output_format = "samples",
    packages = c("./utils")
    )
}

runModelHMC <- function(model, data_to_webppl, 
                         numSamples = 50000, stepSize = 0.05) {
  wp <- webppl(
    program_code = model,
    data = data_to_webppl,
    data_var = "observed_data",
    inference_opts = list(method="MCMC", 
                             kernel = list(HMC = 
                                      list(steps = 10,
                                           stepSize = stepSize)),
                          samples = numSamples, 
                          burn = numSamples/2,
                          verbose = TRUE),
    model_var = "model",
    output_format = "samples",
    packages = c("./utils")
  )
}
```

```{r}

model1 <- '
var linearFunction = function(m, x, b){
  return m*x + b
}

var model = function() {
  var m = gaussian(0, 10)
  var b = gaussian(0, 10)
  var sigma = uniform(0, 20)

  foreach(observed_data, function(dataPoint){
    var x = dataPoint["x"];
    var y = dataPoint["y"];
    var predictedY = linearFunction(m, x, b)
    observe({
      data: y,
      link: Gaussian({mu: predictedY, sigma: sigma})
    })
  })

  return {m: m, b: b, sigma: sigma};
}
'

numSamples = 50000
observed_data <- data.frame(x = c(0,1,2,3), y = c(0,1,4,6));
res <- runModelMCMC(model1, observed_data, numSamples = numSamples) 

res %>%
  gather(parameter, value) %>%
  ggplot(aes(x = value))+
    geom_histogram()+
    facet_wrap(~parameter, scales = 'free')

res %>% 
  gather(parameter, value) %>%
  group_by(parameter) %>%
  summarize(mode = estimate_mode(value),
            md_lo = round(HPDlo(value), 3),
            md_hi = round(HPDhi(value), 3))
```

### Gibson & Wu (2012) -- Fixed effects

Now that we understand a what a simple linear regression looks like from a Bayesian viewpoint, we are ready to turn to the mixed-model example from Sorensen et al (2016). In this tutorial, we build up a progressively more complex Bayesian mixed-effects model on a real-world data set. 

The simplest model is a purely fixed effects model, where we assume that all data points are independent (i.e. have uncorrelated errors). The only additional change from the model above is that we assume reaction times are *log-normal*, meaning that the logarithm of the reaction times follows a normal distribution. We do this simply by taking the log of the input reaction times. 

(Note that for this data set, where each individual gives responses for many different items, a purely fixed-effects model will be a bad assumption. We will handle correlated errors in a mixed-model further below.)

```{r}
fixedEffectModel <- '
var linearFunction = function(x, b_0, b_1){
  return b_1*x + b_0
}

var model = function() {
  var b_0 = sample(UniformDrift({a: -10, b: 10, r:.1}))
  var b_1 = sample(UniformDrift({a: -10, b: 10, r:.1}))
  var sigma = sample(UniformDrift({a: 0, b: 10, r:.1}))

  foreach(observed_data, function(dataPoint){
    var x = dataPoint["so"];
    var y = Math.log(dataPoint["rt"]);
    var predictedY = linearFunction(x, b_0, b_1)
    observe({
      data: y,
      link: Gaussian({mu: predictedY, sigma: sigma})
    })
  })

  return {b_0: b_0, b_1: b_1, sigma: sigma};
}
'

numSamples = 10000
GibsonWuData = read.table("../data/gibsonwu2012data.txt", header = T) %>%
  filter(region == "headnoun") %>%
  mutate(subj = as.integer(factor(subj)),
         item = as.integer(factor(item)),
         so = ifelse(type == "subj-ext", -1, 1)) %>%
  select(subj, item, so, rt)

res <- runModelMCMC(fixedEffectModel, GibsonWuData, numSamples = numSamples)

res %>%
  gather(parameter, value) %>%
  ggplot(aes(x = value))+
    geom_histogram()+
    facet_wrap(~parameter, scales = 'free')

res %>% 
  gather(parameter, value) %>%
  group_by(parameter) %>%
  summarize(mode = estimate_mode(value),
            md_lo = round(HPDlo(value), 3),
            md_hi = round(HPDhi(value), 3))
```

### Gibson & Wu (2012) -- Random intercepts

The problem with a pure fixed-effects model in this data set is that our observations are clustered in a natural way: we suspect that multiple observations from the same person or for the same item may be correlated. This violates the assumption of uncorrelated errors we made above. For example, some people may just be slower than other people, or some items may just be harder. To account for this variance, we add subject-level and item-level intercepts. 

Note that the Bayesian approach explicitly uses a generative model. Try writing the graphical model for this model. We start with sigmas giving the variability across the different intercepts, sample a set of intercepts with these sigmas, sample a set of fixed effects, and get a score for how likely the actual data would be with this set of choices. Over many samples, we can find distributions of values that make the data most likely, given our (relatively uninformative) priors.


```{r}
varyingInterceptModel <- '
// TODO: extract this cleanly from the data...
var numSubjects = 37;
var numItems = 15;

var model = function() {
  // Fixed slope & intercept 
  var b_0 = sample(UniformDrift({a: 3, b: 8, r:.05}))
  var b_1 = sample(UniformDrift({a: -3, b: 3, r:.05}))

  // Error sd
  var sigma_error = sample(UniformDrift({a: 0, b: 1, r:.01}))

  // Subject intercepts
  var sigma_subj = sample(UniformDrift({a: 0, b: 1, r:.01}))
  var subj_intercepts = repeat(numSubjects, function() {
    return sample(Gaussian({mu: 0, sigma: sigma_subj}))
  })

  // Item intercepts
  var sigma_item = sample(UniformDrift({a: 0, b: 1, r:.01}))
  var item_intercepts = repeat(numItems, function() {
    return sample(Gaussian({mu: 0, sigma: sigma_item}))
  })

  foreach(observed_data, function(dataPoint){
    var subjIntercept = subj_intercepts[dataPoint["subj"] - 1]
    var itemIntercept = item_intercepts[dataPoint["item"] - 1]
    var x = dataPoint["so"];
    var y = Math.log(dataPoint["rt"]);
    var predictedY = (b_0 + subjIntercept + itemIntercept) + b_1 * x
    observe({
      data: y,
      link: Gaussian({mu: predictedY, sigma: sigma_error})
    })
  })

  return {b_0: b_0, b_1: b_1, 
          sigma_error: sigma_error,
          sigma_item : sigma_item,
          sigma_subj : sigma_subj};
}
'

numSamples = 50000

res <- runModelMCMC(varyingInterceptModel, GibsonWuData, 
                    numSamples = numSamples)

res %>%
  gather(parameter, value) %>%
  ggplot(aes(x = value))+
    geom_histogram()+
    facet_wrap(~parameter, scales = 'free')

res %>% 
  gather(parameter, value) %>%
  group_by(parameter) %>%
  summarize(mode = estimate_mode(value),
            md_lo = round(HPDlo(value), 3),
            md_hi = round(HPDhi(value), 3))
```



```{r}
'var xs = [-10, -5, 2, 6, 10]
var labels = [false, false, true, true, true]

var model = function() {
  var m = gaussian(0, 1)
  var b = gaussian(0, 1)
  var sigma = gamma(1, 1)

  var y = function(x) {
    return gaussian(m * x + b, sigma)
  }

  var sigmoid = function(x) {
    return 1 / (1 + Math.exp(-1 * y(x)))
  }

  map2(
      function(x, label) {
        factor(Bernoulli({p: sigmoid(x)}).score(label))
      },
      xs,
      labels)

  return sigmoid(8)
}
'
```

