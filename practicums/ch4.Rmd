---
title: "W&L ch4"
author: "mht"
date: "October 15, 2014"
output: html_document
---

This chapter is concerned with inferring the parameters of a Gaussian distribution. 

```{r 4.1.1a}
library(rwebppl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(jsonlite)

getwd() # check working directory
setwd(...) # change working directory if need be

rm(list=ls()) # clear workspace

runModelMCMC <- function(model, data_to_webppl, 
                         numSamples = 50000) {
  wp <- webppl(
    program_code = model,
    data = data_to_webppl,
    data_var = "observed_data", 
       inference_opts = list(method="MCMC", 
                            samples = numSamples, 
                            burn = numSamples/2,
                            verbose = TRUE),
    model_var = "model",
    output_format = "samples",
    packages = c("./utils")
    )
}

runModelHMC <- function(model, data_to_webppl, 
                         numSamples = 50000, stepSize = 0.05) {
  wp <- webppl(
    program_code = model,
    data = data_to_webppl,
    data_var = "observed_data",
    inference_opts = list(method="MCMC", 
                             kernel = list(HMC = 
                                      list(steps = 10,
                                           stepSize = stepSize)),
                          samples = numSamples, 
                          burn = numSamples/2,
                          verbose = TRUE),
    model_var = "model",
    output_format = "samples",
    packages = c("./utils")
  )
}
```

# 4.1 Inferring a mean and standard deviation

```{r}
model41 <- '
var model = function() {

  var mu = sample(Gaussian({mu: 0, sigma: 30}));
  var sigma = sample(Uniform({a:0, b:10}));

  observe({
    data: observed_data,
    link: Gaussian({mu: mu, sigma: sigma})
  })

  return {mu: mu, sigma: sigma}
}
'
```

# 4.1.1

Try a few data sets, varying what you expect the mean and standard deviation to be, and how many data you observe. 

Some suggestions:

(1) Gaussian generated data (HINT: use `data <- 5 + 2 * rnorm(4)` for 4 data points with underlying mu = 5  and sigma = 2)
(2) a longer list of data (e.g. lenght 10 instead of length 4)
(3) uniformly generated data (HINT: use `data <- 1.5 * runif(30)`)

Also, try varying numSamples (fixing the data).


```{r}
dataToUse <- 5 + 2 * rnorm(4)
print(dataToUse)

numSamples <- 2000

res <- runModelHMC(model41, dataToUse, numSamples = numSamples)

# examine res
head(res) 

# Plot sample histograms
res %>%
  gather(parameter, value, mu, sigma) %>%
  ggplot(aes(x = value))+
    geom_histogram()+
    facet_wrap(~parameter, scales = 'free')

```

# 4.1.2

Plot the joint posterior of mu and sigma. That is, plot the samples from
mu against those of sigma. Interpret the shape of the joint posterior.

```{r}
head(res)

# Fill in the columns you want to plot...
ggplot(res, aes(x = ..., y = ...)) +
  geom_point()
```

Interpret:

### 4.1.3

Suppose you knew the standard deviation of the Gaussian was 1.0, but still wanted to infer the mean from data. This is a realistic question: For example, knowing the standard deviation might amount to knowing the noise associated with measuring some psychological trait using a test instrument. The xi values could then be repeated measures for the same person, and their mean the trait value you are trying to infer. Modify the model to do this. What does the revised graphical model look like?

```{r 4.1.3}
model41FixedSigma <- '
var model = function() {

  var mu = sample(Gaussian({mu: 0, sigma: 30}));
  var sigma = ... // SET SIGMA TO A CONSTANT HERE

  observe({
    data: observed_data,
    link: Gaussian({mu: mu, sigma: sigma})
  })

  return {mu: mu, sigma: sigma}
}
'

dataToUse <- 5 + 1 * rnorm(10)
mean(dataToUse)

numSamples <- 5000

res <- runModelHMC(model41FixedSigma, dataToUse, 
                   numSamples)

head(res) # examine res
res %>%
  gather(parameter, value, mu, sigma) %>%
  ggplot(aes(x = value))+
    geom_histogram()+
    facet_wrap(~parameter, scales = 'free')

```

### 4.1.4

Suppose you knew the mean of the Gaussian was zero, but wanted to infer the standard deviation from data. This is also a realistic question: Suppose you know that the error associated with a measurement is unbiased, so its average or mean is zero, but you are unsure how much noise there is in the instrument. Inferring the standard deviation is then a sensible way to infer the noisiness of the instrument. Once again, modify the model. Once again, what does the revised graphical model look like?

```{r 4.1.3}
model41FixedMu <- '
var model = function() {
  var mu = 0 // SET MU TO A CONSTANT HERE
  var sigma = sample(Uniform({a:0, b:10}));

  observe({
    data: observed_data,
    link: Gaussian({mu: mu, sigma: sigma})
  })

  return {mu: mu, sigma: sigma}
}
'

dataToUse <- 0 + 5 * rnorm(10)
sd(dataToUse)

numSamples <- 5000

res <- runModelHMC(model41FixedMu, dataToUse, numSamples)

res %>%
  gather(parameter, value, mu, sigma) %>%
  ggplot(aes(x = value))+
    geom_histogram()+
    facet_wrap(~parameter, scales = 'free')
```

### 4.2 The seven scientists

Seven scientists with wildly-differing experimental skills all make a measurement of the same quantity. They get the answers x = {−27.020, 3.570, 8.191, 9.898, 9.603, 9.945, 10.056}. Intuitively, it seems clear that the first two scientists are pretty inept measurers, and that the true value of the quantity is probably just a bit below 10. The main problem is to find the posterior distribution over the measured quantity, telling us what we can infer from the measurement. A secondary problem is to infer something about the measurement skills of the seven scientists.

# 4.2.1

```{r}
sevenScientistsModel <- '
// notice below that toJSON packages this up as object inside of an array
// so here we grab the 1st element of the array (the object)
var data = observed_data[0] 

var scientist_names = [
    "scientist_1","scientist_2", "scientist_3","scientist_4",
    "scientist_5","scientist_6","scientist_7"
];

// Here were using uniform priors on sigma (rather than the inverse gamma in LW)
var sampleSigma = function(){ return sample(UniformDrift({a:0, b:20, r:1})); }

// To use the inverse-gamma from LW;
// Note that webppl uses shape & scale instead of shape and rate
// var sampleSigma = function(){ return 1/Math.sqrt(gamma(0.1, 1/0.1)); }

var model = function() {

  var mu = sample(GaussianDrift({mu: 0, sigma: 30}));

  var sigmas = {
    scientist_1 : sampleSigma(),
    scientist_2 : sampleSigma(),
    scientist_3 : sampleSigma(),
    scientist_4 : sampleSigma(),
    scientist_5 : sampleSigma(),
    scientist_6 : sampleSigma(),
    scientist_7 : sampleSigma()
  }
  
  // foreach is in utils packages: it is like map, but doesnt return anything
  foreach(scientist_names, function(scientist){
        observe({
          data: data[scientist],
          link: Gaussian({mu: mu, sigma: sigmas[scientist]})
        })
  })

  return utils.extend({mu:mu}, sigmas)
}
'

observed_data <- data.frame(
  scientist_1 = -27.020, 
  scientist_2 = 3.570, 
  scientist_3 = 8.191, 
  scientist_4 = 9.898, 
  scientist_5 = 9.603, 
  scientist_6 = 9.945, 
  scientist_7 = 10.056
  )
  
print(observed_data)

toJSON(observed_data) # use toJSON to see how your data will look in WebPPL

numSamples <- 100000

res <- runModelMCMC(sevenScientistsModel, observed_data, numSamples)

res %>%
  gather(parameter, value) %>%
  ggplot(aes(x = value))+
    geom_histogram()+
    facet_wrap(~parameter, scales = 'free')
```

# 4.2.2 

Experiment with the effect the upper bound of this uniform prior has on inference.

```{r 4.2.2}
sevenScientistsModel <- '
var data = observed_data[0] 

var scientist_names = [
    "scientist_1","scientist_2","scientist_3","scientist_4",
    "scientist_5","scientist_6","scientist_7"
];

// CHANGE UPPER BOUND b HERE
var sampleSigma = function(){ 
  return sample(UniformDrift({a:0, b: ..., r:1})); 
};

// To use the inverse-gamma from LW;
// Note that webppl uses shape & scale instead of shape and rate
// var sampleSigma = function(){ return 1/Math.sqrt(gamma(0.1, 1/0.1)); }

var model = function() {

  var mu = sample(GaussianDrift({mu: 0, sigma: 30}));

  var sigmas = {
    scientist_1 : sampleSigma(),
    scientist_2 : sampleSigma(),
    scientist_3 : sampleSigma(),
    scientist_4 : sampleSigma(),
    scientist_5 : sampleSigma(),
    scientist_6 : sampleSigma(),
    scientist_7 : sampleSigma()
  }
  
  // foreach is in utils packages: it is like map, but doesnt return anything
  foreach(scientist_names, function(scientist){
        observe({
          data: data[scientist],
          link: Gaussian({mu: mu, sigma: sigmas[scientist]})
        })
  })

  return utils.extend({mu:mu}, sigmas)
}
'

observed_data <- data.frame(
  scientist_1 = -27.020, 
  scientist_2 = 3.570, 
  scientist_3 = 8.191, 
  scientist_4 = 9.898, 
  scientist_5 = 9.603, 
  scientist_6 = 9.945, 
  scientist_7 = 10.056
  )

numSamples <- 100000

res <- runModelMCMC(sevenScientistsModel, observed_data, numSamples)

res %>%
  gather(parameter, value) %>%
  ggplot(aes(x = value))+
    geom_histogram()+
    facet_wrap(~parameter, scales = 'free')
```

### 4.3 Repeated measures of IQ

The data are the measures xij for the i = 1, . . . , n people and their j = 1, . . . , m repeated test scores.

We assume that the differences in repeated test scores are distributed as Gaussian error terms with zero mean and unknown precision. The mean of the Gaussian of a person’s test scores corresponds to their latent true IQ. This will be different for each person. The standard deviation of the Gaussians corresponds to the accuracy of the testing instruments in measuring the one underlying IQ value. We assume this is the same for every person, since it is conceived as a property of the tests themselves.

```{r}
IQmodelUniformPrior <- '
var model = function() {
  // single SD for all people (corresponding to measurement error)
  var data = [observed_data["p1"], observed_data["p2"], observed_data["p3"]];
  var sigma = uniform(0,100)

  // each person has a latent IQ; there are different priors we could set here...
  var mus = repeat(data.length, function() {return uniform(0, 300)})

  var score = sum(map(function(IQPair) {
    var measurements = IQPair[0]
    var mu = IQPair[1]
    return sum(map(function(measurement) {
      return Gaussian({mu: mu, sigma: sigma}).score(measurement);
    }, measurements));
  }, zip(data, mus)))
  factor(score)
  return {mu1: mus[0], mu2: mus[1], mu3: mus[2], sigma: sigma}
}
'
```

# 4.3.1 

Use the posterior distribution for each person’s mu_i to estimate their IQ. What can we say about the precision of the IQ test?

```{r}
# Three people with three measurements each; try different values...
modelToUse <- IQmodelUniformPrior
dataToUse <- list(p1 = c(90,95,100), p2 = c(105,110,115), p3 = c(150,155,160))

res <- webppl(
    program_code = modelToUse,
    data = dataToUse,
    data_var = "observed_data",
    inference_opts = list(method = "incrementalMH", samples = 50000, 
                          burn = 1000, verbose = TRUE),
    model_var = "model",
    packages = c("./utils")
    )

res.samp = get_samples(res, num_samples = 50000) %>% 
  gather(param, val, mu1: sigma) 
  
ggplot(res.samp, aes(x = val)) +
  geom_histogram() +
  facet_wrap( ~ param, scale =  "free")
```

# 4.3.2 

Now, use a more realistic prior assumption for the μi means. Theoretically, IQ distributions should have a mean of 100, and a standard deviation of 15. Make this change in the script, and re-run the inference. How do the estimates of IQ given by the means change? Why?

```{r}
IQmodelGaussianPrior <- '
var model = function() {
  // single SD for all people (corresponding to measurement error)
  var data = [observed_data["p1"], observed_data["p2"], observed_data["p3"]];
  var sigma = uniform(0,100)

  // each person has a latent IQ; there are different priors we could set here...
  var mus = repeat(data.length, function() {return gaussian(100, 15);})

  var score = sum(map(function(IQPair) {
    var measurements = IQPair[0]
    var mu = IQPair[1]
    return sum(map(function(measurement) {
      return Gaussian({mu: mu, sigma: sigma}).score(measurement);
    }, measurements));
  }, zip(data, mus)))
  factor(score)
  return {mu1: mus[0], mu2: mus[1], mu3: mus[2], sigma: sigma}
}
'
```

```{r}
# Three people with three measurements each; try different values...
modelToUse <- IQmodelGaussianPrior
dataToUse <- list(p1 = c(90,95,100), p2 = c(105,110,115), p3 = c(150,155,160))

res <- webppl(
    program_code = modelToUse,
    data = dataToUse,
    data_var = "observed_data",
    inference_opts = list(method = "incrementalMH", samples = 100000, burn = 1000),
    model_var = "model",
    packages = c("./utils")
    )

res.samp = get_samples(res, num_samples = 100000) %>% 
  gather(param, val, mu1: sigma) 
  
ggplot(res.samp, aes(x = val)) +
  geom_histogram() +
  facet_wrap( ~ param, scale =  "free")

```

# 4.3.3. 

Repeat both of the above stages (i.e., using both priors on μ_i) with a new, but closely related, data set that has scores of (94, 95, 96), (109, 110,111), and (154,155,156). How do the different prior assumptions affect IQ estimation for these data. Why does it not follow the same pattern as the previous data?

```{r}
# Three people with three measurements each; try different values...
modelToUse <- IQmodelUniformPrior 
# modelToUse <- IQmodelGaussianPrior 
dataToUse <- list(p1 = c(94, 95, 96), p2 = c(109, 110,111), p3 = c(154,155,156))

numSamples = 10000;

res.samp <- webppl(program_code = modelToUse,
       data = dataToUse,
       data_var = "observed_data",
       inference_opts = list(method="MCMC", 
                             kernel = 
                               list(HMC = 
                                      list(steps = 10,
                                           stepSize = 0.1)),
                          samples = numSamples, 
                          burn = numSamples/2,
                          verbose = TRUE),
      model_var = "model",
      packages = c("./utils"),
      output_format = "samples"
      )

res.samp %>%
  gather(parameter, val) %>%
  ggplot(aes(x = val)) +
  geom_histogram() +
  facet_wrap( ~ parameter, scale =  "free")

```