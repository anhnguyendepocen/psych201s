---
title: "W&L ch4"
author: "mht"
date: "October 15, 2014"
output: html_document
---

This chapter is concerned with inferring the parameters of a Gaussian distribution. 

# 4.1 Inferring a mean and standard deviation

```{r 4.1.1a}
library(R2jags)
library(gridExtra)
library(reshape2)
library(dplyr)
library(ggplot2)
library(tidyr)

setwd("~/Repos/webppl-bcm/")

# clears workspace:  
rm(list=ls()) 

x <- c(1.1, 1.9, 2.3, 1.8)
n <- length(x)

data <- list("x", "n") # to be passed on to JAGS
myinits <- list(
  list(mu = 0, sigma = 1))

# parameters to be monitored:  
parameters <- c("mu", "sigma")

cat('# Inferring the Mean and Standard Deviation of a Gaussian
model{
  # Data Come From A Gaussian
  for (i in 1:n){
    x[i] ~ dnorm(mu,lambda)
  }
  # Priors
  mu ~ dnorm(0,.001)
  sigma ~ dunif(0,10)
  lambda <- 1/pow(sigma,2)
}',file={gaussn <- tempfile()})

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples <- jags(data, inits=myinits, parameters,
	 			 model.file=gaussn, n.chains=1, n.iter=1000, 
         n.burnin=1, n.thin=1, DIC=T)
# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.
df<-data.frame(mu = samples$BUGSoutput$sims.list$mu, 
           sigma = samples$BUGSoutput$sims.list$sigma)

blankPanel<-grid.rect(gp=gpar(col="white"))
a<-qplot(data=df,x=mu,y=sigma,geom='point')+theme_bw()+xlim(0,3)+ylim(0,3)
b<-qplot(mu, data=df, geom='histogram',binwidth=0.1)+theme_bw()+xlim(0,3)
c<-qplot(sigma, data=df, geom='histogram',binwidth=0.1)+theme_bw()+
  coord_flip()+scale_y_reverse()+xlim(0,3)

#quartz("data")
grid.arrange(a,c,b,blankPanel,ncol=2)

```

Same thing but with 3x as much data.

```{r 4.1.1b}

x <- c(1.1, 1.9, 2.3, 1.8, 1.1, 1.9, 2.3, 1.8, 1.1, 1.9, 2.3, 1.8)
n <- length(x)

data <- list("x", "n") # to be passed on to JAGS
myinits <- list(
  list(mu = 0, sigma = 1))

parameters <- c("mu", "sigma")

samples <- jags(data, inits=myinits, parameters,
   			 model.file=gaussn, n.chains=1, n.iter=1000, 
         n.burnin=1, n.thin=1, DIC=T)

df<-data.frame(mu = samples$BUGSoutput$sims.list$mu, 
           sigma = samples$BUGSoutput$sims.list$sigma)

blankPanel<-grid.rect(gp=gpar(col="white"))
a<-qplot(data=df,x=mu,y=sigma,geom='point')+theme_bw()+xlim(0,3)+ylim(0,3)
b<-qplot(mu, data=df, geom='histogram',binwidth=0.1)+theme_bw()+xlim(0,3)
c<-qplot(sigma, data=df, geom='histogram',binwidth=0.1)+theme_bw()+
  coord_flip()+scale_y_reverse()+xlim(0,3)

#quartz("3x data")
grid.arrange(a,c,b,blankPanel,ncol=2)

```

And now with uniform data

```{r 4.1.1c}
  
x <- 1.5*runif(30)
n <- length(x)

data <- list("x", "n") # to be passed on to JAGS
myinits <- list(
  list(mu = 0, sigma = 1))

parameters <- c("mu", "sigma")

samples <- jags(data, inits=myinits, parameters,
     		 model.file=gaussn, n.chains=1, n.iter=1000, 
         n.burnin=1, n.thin=1, DIC=T)

df<-data.frame(mu = samples$BUGSoutput$sims.list$mu, 
           sigma = samples$BUGSoutput$sims.list$sigma)

blankPanel<-grid.rect(gp=gpar(col="white"))
a<-qplot(data=df,x=mu,y=sigma,geom='point')+theme_bw()+xlim(0,3)+ylim(0,3)
b<-qplot(mu, data=df, geom='histogram',binwidth=0.1)+theme_bw()+xlim(0,3)
c<-qplot(sigma, data=df, geom='histogram',binwidth=0.1)+theme_bw()+xlim(0,3)+
  coord_flip()+scale_y_reverse()

#quartz("uniform random data")
grid.arrange(a,c,b,blankPanel,ncol=2)

```



### 4.1.3

Suppose you knew the standard deviation of the Gaussian was 1.0, but still wanted to infer the mean from data. This is a realistic question: For example, knowing the standard deviation might amount to knowing the noise associated with measuring some psychological trait using a test instrument. The xi values could then be repeated measures for the same person, and their mean the trait value you are trying to infer. Modify the WinBUGS script and Matlab or R code to do this. What does the revised graphical model look like?


```{r 4.1.3}

x <- c(1.1, 1.9, 2.3, 1.8)
n <- length(x)

data <- list("x", "n") # to be passed on to JAGS
myinits <- list(
  list(mu = 0))

parameters <- c("mu")

cat('# Inferring the Mean and Standard Deviation of a Gaussian
model{
  # Data Come From A Gaussian
  for (i in 1:n){
    x[i] ~ dnorm(mu,lambda)
  }
  # Priors
  mu ~ dnorm(0,.001)
  sigma <- 1
  lambda <- 1/pow(sigma,2)
}',file={gaussn.mean<-tempfile()})

samples <- jags(data, inits=myinits, parameters,
       	 model.file=gaussn.mean, n.chains=1, n.iter=1000, 
         n.burnin=1, n.thin=1, DIC=T)

df<-data.frame(mu = samples$BUGSoutput$sims.list$mu)

#quartz("mean only")
qplot(mu, data=df, geom='histogram',binwidth=0.1)+theme_bw()+xlim(0,3)

```

### 4.1.4

Suppose you knew the mean of the Gaussian was zero, but wanted to infer the standard deviation from data. This is also a realistic question: Suppose you know that the error associated with a measurement is unbiased, so its average or mean is zero, but you are unsure how much noise there is in the instrument. Inferring the standard deviation is then a sensible way to infer the noisiness of the instrument. Once again, modify the WinBUGS script and Matlab or R code to do this. Once again, what does the revised graphical model look like?

```{r 4.1.4}

x <- c(1.1, 1.9, 2.3, 1.8)
n <- length(x)

data <- list("x", "n") # to be passed on to JAGS
myinits <- list(
  list(sigma = 1))

parameters <- c("sigma")

cat('# Inferring the Mean and Standard Deviation of a Gaussian
model{
  # Data Come From A Gaussian
  for (i in 1:n){
    x[i] ~ dnorm(mu,lambda)
  }
  # Priors
  mu <- 0
  sigma ~ dunif(0,10)
  lambda <- 1/pow(sigma,2)
}',file={gaussn.var<-tempfile()})

samples <- jags(data, inits=myinits, parameters,
       	 model.file=gaussn.var, n.chains=1, n.iter=1000, 
         n.burnin=1, n.thin=1, DIC=T)

df<-data.frame(sigma = samples$BUGSoutput$sims.list$sigma)
#quartz("sigma only")
qplot(sigma, data=df, geom='histogram',binwidth=0.1)+theme_bw()+xlim(0,3)

```


### 4.2 The seven scientists

Seven scientists with wildly-differing experimental skills all make a measurement of the same quantity. They get the answers x = {−27.020, 3.570, 8.191, 9.898, 9.603, 9.945, 10.056}. Intuitively, it seems clear that the first two scientists are pretty inept measurers, and that the true value of the quantity is probably just a bit below 10. The main problem is to find the posterior distribution over the measured quantity, telling us what we can infer from the measurement. A secondary problem is to infer something about the measurement skills of the seven scientists.


```{r 4.2.1}
x <- c(-27.020,3.570,8.191,9.898,9.603,9.945,10.056)
n <- length(x)

data <- list("x", "n")
myinits <- list(
  list(mu = 0, lambda = rep(1,n)))

parameters <- c("mu", "sigma")

cat('# The Seven Scientists
model{
  # Data Come From Gaussians With Common Mean But Different Precisions
  for (i in 1:n){
    x[i] ~ dnorm(mu,lambda[i])
  }
  # Priors
  mu ~ dnorm(0,.001)
  for (i in 1:n){
    lambda[i] ~ dgamma(.001,.001)
    sigma[i] <- 1/sqrt(lambda[i])  
  }     
}',file={seven.scientists<-tempfile()})

samples <- jags(data, inits=myinits, parameters,
	 			 model.file =seven.scientists, n.chains=1, n.iter=1000, 
         n.burnin=1, n.thin=1, DIC=T)

df_sigmas<-data.frame(samples$BUGSoutput$sims.list$sigma)
df_mean<-data.frame(mu = samples$BUGSoutput$sims.list$mu)


a<-ggplot(melt(df_sigmas),aes(x=value))+
  facet_wrap(~variable,scales='free')+
  geom_histogram(binwidth=1)+
  theme_bw()+
  xlab('sigma')+
  xlim(0, 50)

b<-qplot(data=df_mean,mu,geom='histogram',binwidth=0.2)+theme_bw()

#quartz("seven scientists")
grid.arrange(a,b,ncol=2)
```



Exercise 4.2.2 Change the graphical model in Figure 4.2 to use a uniform prior over the standard deviations, as was done in Figure 4.1. Experiment with the effect the upper bound of this uniform prior has on inference.



```{r 4.2.2}
x <- c(-27.020,3.570,8.191,9.898,9.603,9.945,10.056)
n <- length(x)

data <- list("x", "n")
myinits <- list(
  list(mu = 0, sigma = rep(5,n)))

parameters <- c("mu", "sigma")

cat('# The Seven Scientists
model{
  # Data Come From Gaussians With Common Mean But Different Precisions
  for (i in 1:n){
    x[i] ~ dnorm(mu,lambda[i])
  }
  # Priors
  mu ~ dnorm(0,.001)
  for (i in 1:n){
    sigma[i] ~ dunif(0,10) 
    lambda[i] <- 1/pow(sigma[i],2)
  }     
}',file={seven.unif<-tempfile()})

samples <- jags(data, inits=myinits, parameters,
   			 model.file=seven.unif, n.chains=1, n.iter=1000, 
         n.burnin=1, n.thin=1, DIC=T)

df_sigmas<-data.frame(samples$BUGSoutput$sims.list$sigma)
df_mean<-data.frame(mu = samples$BUGSoutput$sims.list$mu)


a<-ggplot(melt(df_sigmas),aes(x=value))+
  facet_wrap(~variable,scales='free')+
  geom_histogram(binwidth=1)+
  theme_bw()+
  xlab('sigma')+
  xlim(0, 50)

b<-qplot(data=df_mean,mu,geom='histogram',binwidth=0.2)+theme_bw()

#quartz("seven scientists, uniform prior over sigma")
grid.arrange(a,b,ncol=2)
```

### 4.3 Repeated measures of IQ

The data are the measures xij for the i = 1, . . . , n people and their j = 1, . . . , m repeated test scores.

We assume that the differences in repeated test scores are distributed as Gaussian error terms with zero mean and unknown precision. The mean of the Gaussian of a person’s test scores corresponds to their latent true IQ. This will be different for each person. The standard deviation of the Gaussians corresponds to the accuracy of the testing instruments in measuring the one underlying IQ value. We assume this is the same for every person, since it is conceived as a property of the tests themselves.

```{r}
IQmodelUniformPrior <- '
var model = function() {
  // single SD for all people (corresponding to measurement error)
  var data = [observed_data["p1"], observed_data["p2"], observed_data["p3"]];
  var sigma = uniform(0,100)

  // each person has a latent IQ; there are different priors we could set here...
  var mus = repeat(data.length, function() {return uniform(0, 300)})

  var score = sum(map(function(IQPair) {
    var measurements = IQPair[0]
    var mu = IQPair[1]
    return sum(map(function(measurement) {
      return Gaussian({mu: mu, sigma: sigma}).score(measurement);
    }, measurements));
  }, zip(data, mus)))
  factor(score)
  return {mu1: mus[0], mu2: mus[1], mu3: mus[2], sigma: sigma}
}
'
```

# 4.3.1 

Use the posterior distribution for each person’s μi to estimate their IQ. What can we say about the precision of the IQ test?

```{r}
# Three people with three measurements each; try different values...
modelToUse <- IQmodelUniformPrior
dataToUse <- list(p1 = c(90,95,100), p2 = c(105,110,115), p3 = c(150,155,160))

res <- webppl(
    program_code = modelToUse,
    data = dataToUse,
    data_var = "observed_data",
    inference_opts = list(method = "incrementalMH", samples = 50000, burn = 1000),
    model_var = "model",
    packages = c("./utils")
    )

res.samp = get_samples(res, num_samples = 50000) %>% 
  gather(param, val, mu1: sigma) 
  
ggplot(res.samp, aes(x = val)) +
  geom_histogram() +
  facet_wrap( ~ param, scale =  "free")
```

# 4.3.2 

Now, use a more realistic prior assumption for the μi means. Theoretically, IQ distributions should have a mean of 100, and a standard deviation of 15. Make this change in the script, and re-run the inference. How do the estimates of IQ given by the means change? Why?

```{r}
IQmodelGaussianPrior <- '
var model = function() {
  // single SD for all people (corresponding to measurement error)
  var data = [observed_data["p1"], observed_data["p2"], observed_data["p3"]];
  var sigma = uniform(0,100)

  // each person has a latent IQ; there are different priors we could set here...
  var mus = repeat(data.length, function() {return gaussian(100, 15);})

  var score = sum(map(function(IQPair) {
    var measurements = IQPair[0]
    var mu = IQPair[1]
    return sum(map(function(measurement) {
      return Gaussian({mu: mu, sigma: sigma}).score(measurement);
    }, measurements));
  }, zip(data, mus)))
  factor(score)
  return {mu1: mus[0], mu2: mus[1], mu3: mus[2], sigma: sigma}
}
'
```

```{r}
# Three people with three measurements each; try different values...
modelToUse <- IQmodelGaussianPrior
dataToUse <- list(p1 = c(90,95,100), p2 = c(105,110,115), p3 = c(150,155,160))

res <- webppl(
    program_code = modelToUse,
    data = dataToUse,
    data_var = "observed_data",
    inference_opts = list(method = "incrementalMH", samples = 100000, burn = 1000),
    model_var = "model",
    packages = c("./utils")
    )

res.samp = get_samples(res, num_samples = 100000) %>% 
  gather(param, val, mu1: sigma) 
  
ggplot(res.samp, aes(x = val)) +
  geom_histogram() +
  facet_wrap( ~ param, scale =  "free")

```

# 4.3.3. 

Repeat both of the above stages (i.e., using both priors on μ_i) with a new, but closely related, data set that has scores of (94, 95, 96), (109, 110,111), and (154,155,156). How do the different prior assumptions affect IQ estimation for these data. Why does it not follow the same pattern as the previous data?

```{r}
# Three people with three measurements each; try different values...
# modelToUse <- IQmodelUniformPrior 
modelToUse <- IQmodelGaussianPrior 
dataToUse <- list(p1 = c(94, 95, 96), p2 = c(109, 110,111), p3 = c(154,155,156))

res <- webppl(
    program_code = modelToUse,
    data = dataToUse,
    data_var = "observed_data",
    inference_opts = list(method = "incrementalMH", samples = 100000, burn = 10000),
    model_var = "model",
    packages = c("./utils")
    )

res.samp = get_samples(res, num_samples = 100000) %>% 
  gather(param, val, mu1: sigma) 
  
ggplot(res.samp, aes(x = val)) +
  geom_histogram() +
  facet_wrap( ~ param, scale =  "free")

```