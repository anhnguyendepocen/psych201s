---
title: "bayes-rule"
author: "mht"
date: "June 14, 2016"
output: html_document
---

In this tutorial, we're going to walk through how to get from a prior belief distribution to a posterior belief distribution through the process of observing data.

```{r}
library(rwebppl)
```

```{r}
myPrior <- "
var prior = function(){
  var p = sample(Uniform({a:0,b:1}))
  return p
}
repeat(10000, prior)
"

webppl(myPrior) %>% qplot()
```


Looks pretty uniform. 
Now let's imagine these are our prior beliefs over the proportion of subjects who will behave prosocially in our experiment (for example, in the experimental condition).

Now we observe 15 out of 20 subjects behave prosocially. 
That is our data. 
How do we go from a prior belief distribution to a posterior belief distribution? 
That is, how do we *update* our beliefs in light of this evidence?

Bayes' Theorem is the mathematically correct way of doing this. 
But rather than do the math, can we program something to do it for us?

We need to be a tiny bit more specific. 
In particular, we need to have a *linking function* that maps from our beliefs to observed data. 
(Here, we need something that goes from `p` -- a coin weight -- to some number of observed outcomes, i.e., the number of times the coin came up heads, or number of people who behaved prosocially.)

The `Binomial` distribution is such a linking function.
It is a mapping from a coin weight to the number of heads.

```{r}
priorPredictive <- "
var priorPredictive = function(){
  var p = sample(Uniform({a:0,b:1}))
  var outcome = sample(Binomial( {n:20, p: p} ))
  return outcome
}
repeat(10000, priorPredictive)
"

webppl(myPrior) %>% qplot(binwidth = 1)
```

We still have a uniform distribution, but note that this now over the numbers of heads, so the x-axis goes from 0 - 20. 
That tells us that when we don't know the weight of the coin, any numbers of heads is equally likely (in addition to all values of the weight of the coin being equally likely).
This distribution is called the *prior predictive* distribution, because it shows what data our model predicts, because it has learned anything from the data.
Here we see that our hypothesis (i.e. our model) is not very specific.
It can predict any data.
(In your head, imagine what the prior predictive distribution looks like for the usual null hypothesis: that the coin weight is 0.5; later in this course, we will talk about comparing models (e.g., the null model vs. this model) as the Bayesian way of doing hypothesis testing. But that is for later.)

So now we're ready to integrate in our data.
The generative way of doing this is to say: 

1. Sample a coin weight from our prior `sample(Uniform({a:0,b:1}))`
2. Sample an outcome, given that coin weight `sample(Binomial( {n:20, p: p} ))`
3a. If our outcome is the same as our data, keep the coin weight.
3b. If the outcomes i different from our data, throw away the coin weight.

In principle, any coin weight that's not 0 **could** give rise to our data (with a coin weighted 0.2, it could produce 15 / 20 heads, though it is unlikely).
It turns out, if you repeat this procedure many times, then this is a true method to compute the posterior distribution.
It is called [Rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling).

So,
4. Repeat many times

How could we write this in WebPPL?

Instead of always returning `p`, let's only return `p` when `outcome==15`. 
That is, only return the coin weights that give rise to our data.
We will do this with a simple `if` statement.
In JavaScript / WebPPL, you can do a shorthand `if .. else` statement using `? .. : .. `.
We will use the `?` syntax throughout the course.

```{r}
myPosterior <- "
var sampleFromPosterior = function(){
  var p = sample(Uniform({a:0,b:1}))
  var outcome = sample(Binomial( {n:20, p: p} ))
  return (outcome == 15) ? p : null 
}
repeat(100000, sampleFromPosterior)
"
posterior <-webppl(myPosterior) 

posterior.Without.NAs <- posterior[!is.na(posterior)] # remove all the NA's

qplot(posterior.Without.NAs)
```

Whoa, that's pretty interesting. 
So the coin weights that are most likely to give rise to our data (15 out of 20 heads) are somewhere around 0.75.
This is our first posterior distribution!
It repesents the probabilies of the true population proportion of heads, given our data (and our prior beliefs, which here, were uniform across all possibilities). 

We can ask things like: Between what 2 numbers are we 95% sure the true proportion lies? This is what's called a *credible interval*. 
(Note that credible intervals carry a different interpretation than that assigned to traditional confidence intervals. Confidence intervals have a a substantially more complicated interpretation. We will discuss the differences more fully later in this course.)

Now that we have the posterior distrbution, we can simply draw some lines in it: Let's look at the points at which 2.5% of the posterior probability mass is to the left and 2.5% of the probability mass is to the right, as well as the point at which 50% of the distribution is to the left and 50% is to the right (i.e., the median).

```{r}
quantile(posterior.Without.NAs, probs = c(0.025, 0.5, 0.975))
```

There are other questions you can ask about the posterior, some of which are highlighted in Wagenmakers, Morey, and Lee (2016) *Bayesian benefits for the pragmatic researcher*.

To conclude this tutorial, I just want to tell you that doing this Bayesian inference trick (the 4 steps outlined above) is common to all of Bayesian analyses. It is the mechanism to use the data you have observed to go from prior beliefs to posterior beliefs.

Because we do it so often, it would be useful if there just a simple `observe` function, that took in our data and our linking function (here, the `Binomial`), and returned to us the posterior distribution. 

Indeed:

```{r}
myPosterior <- "
var ourModel = function(){
  var p = sample(Uniform({a:0, b:1}))
  observe({ data : 15,  link : Binomial( {n:20, p: p} ) })
  return p
}
"

posterior <- webppl(
  program_code = myPosterior,
  model_var = "ourModel",
  inference_opts = list(method = "rejection", samples = 10000),
  packages = c("./utils"),
  output_format = "samples"
)
```

These models will do exactly the same thing. The only difference is that we've abstracted the `repeat(10000,..)` and the if `?` statement away.
We also no longer are generating the `outcome` variable explicitly.
In their place, we're calling a function called `observe`, which we're giving our `data: 15` and how we would generate the data (our linking funciton) `link: Binomial(...)`.
We then ask the model to `return p`.

We thus abstract the inference procedure, and using `inference_opts` in the `webppl` call to tell it to do Rejection Sampling with 10000 samples. 
We also need to tell it to do Rejection sampling on `ourModel`.

The `observe` function is a helper function that we've written; we are accessing it through the package `utils`, which is also included in the webppl program call. 
Finally, we've told webppl we want the ouput format in `"samples"` (the default is for a probability table, which we saw in `distributions-wppl.Rmd`).
Since we only have one column, we can pass it to `qplot` and hope that it does the right thing.

```{r}
head(posterior)


qplot(posterior)
```
