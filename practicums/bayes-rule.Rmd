---
title: "bayes-rule"
author: "mht"
date: "June 14, 2016"
output: html_document
---

```{r}
library(rwebppl)
library(ggplot2)
library(tidyr)
library(dplyr)
```

The major advancement introduced by the Reverand Thomas Bayes was the ability to do **inverse inference**: to use *observable* (or observed) data to learn about *unobservable* quantities (e.g., the true population mean). The basic recipe is to start with **prior beliefs** and to state explicitly how these beliefs map to observable data. (This second part is called the **likelihood**, and is something both frequentists and Bayesians agree upon. Frequentists just don't believe in prior beliefs.) The functional reason to accept priors is because we would like to say something about the **posterior beliefs**, that is, after observing this data, what is the probability of my hypothesis?

In this tutorial, we're going to walk through how to get from a prior belief distribution to a posterior belief distribution through the process of observing data.

We'll start with the most canonical prior belief distribution: a uniform distribution over the numbers 0 and 1 (oft interpreted as the weight of a coin, or the true population proportion of doing X in my experiment).

```{r}
myPrior <- "
var prior = function(){
  var p = sample(Uniform({a:0,b:1}))
  return p
}
repeat(10000, prior)
"

samples <- webppl(myPrior) ## run webppl

head(samples) ## print out the first few samples

qplot(samples) ## make histogram of samples
```

Looks pretty uniform (the endpoints are lower because of binning). Now let's imagine these are our prior beliefs over the true population proportion of subjects who will behave prosocially in our experiment (for example, in the experimental condition).

Now, suppose we observe 15 out of 20 subjects behave prosocially. That is our data. How can we incorporate this evidence into our beliefs about the true population mean? How do we go from a prior belief distribution to a posterior belief distribution? That is, how do we *update* our beliefs in light of this evidence?

Bayes' Theorem is the mathematically correct way of doing this. But rather than do the math, can we program something to do it for us?

Well, we need to specify one more thing: *the likelihood*. In Psychology, this also goes under the name of a *linking function*, or *generative model of the data*: it's something that maps our prior beliefs to observed data. (Here, we need something that goes from `p` -- a coin weight -- to observed outcomes, i.e., the number of heads, or number of participants who behaved prosocially.)

The `Binomial` distribution is such a linking function. It is a mapping from a coin weight (and some number of flips) to the number of heads.

```{r}
priorPredictive <- "
var priorPredictive = function(){
  var p = sample(Uniform({a:0,b:1}))
  var outcome = sample(Binomial( {n:20, p: p} )) // linking function
  return outcome
}
repeat(10000, priorPredictive)
"

samples <- webppl(priorPredictive)  ## run webppl

head(samples) ## look at first few samples

qplot(samples, binwidth = 1) ## make histogram of samples
```

We still have a uniform distribution, but note that this now over the numbers of heads, so the x-axis goes from 0 - 20. That tells us that, a priori, we are assuming nothing about the numbers of heads that we'll observe (or the number of participants that are going to behave prosocially in our experiment). This is called an *uninformative prior* (or, ignorance prior)... it doesn't bias our posterior distribution one way or another. This distribution is called the *prior predictive* distribution, because it shows what data our model predicts, before it has seen any data.

We can think of this model as our hypothesis, and it is a vague hypothesis. (It corresponds to the standard alterantive hypothesis: p!=0.5) It plausibly predicts any data. (However, when later in the course discuss "model comparison", which is the same thing as hypothesis testing, we will see that this hypothesis is penalized for being vague compared to the null hypotehsis.) In your head, try to imagine what the prior predictive distribution looks like for the usual null hypothesis: that the coin weight is 0.5. Then, go and change `p` to be 0.5 (instead of sampling it from a uniform).

Is it uniform? Why or why not?

So now we're ready to integrate in our data.
One very simple way of doing this is to say: 

1. Sample a coin weight from our prior `sample(Uniform({a:0,b:1}))`
2. Make a prediction (i.e., sample an outcome), given that coin `sample(Binomial( {n:20, p: p} ))` 
3a. If our prediction matches our data, keep the coin.
3b. If our prediction doesn't match our data, throw away the coin.

In principle, any coin weight that's not exactly 0 **could** give rise to our data (with a coin weighted 0.2, it could produce 15 / 20 heads, though it is unlikely). It turns out, if you repeat this procedure many times, then you end up with the correct posterior distribution on coin weights. It is called [Rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling).

So,
4. Repeat many times

This is an algorithm for doing *Bayesian inference* (going from a prior belief distribution to a posterior belief distribution).

How could we write this in WebPPL?

Instead of always returning `p`, let's only return `p` when `outcome==15`.That is, only return the coin weights predict our data. We will do this with a simple `if` statement. In JavaScript / WebPPL, you can do a shorthand `if .. else` statement using `? .. : .. `. We will use the `?` syntax throughout the course.

```{r}
myPosterior <- "
var sampleFromPosterior = function(){
  var p = sample(Uniform({a:0,b:1}))
  var outcome = sample(Binomial( {n:20, p: p} ))
  return (outcome == 15) ? p : null 
}
repeat(100000, sampleFromPosterior)
"
posterior <- webppl(myPosterior) 

posterior.Without.NAs <- posterior[!is.na(posterior)] ## remove all the NAs

qplot(posterior.Without.NAs)
```

Whoa, that's pretty interesting.  So the coin weights that are most likely to give rise to our data (15 out of 20 heads) are somewhere around 0.75. This is our first posterior distribution! It repesents our beliefs about  the true population proportion of heads, given our data (and our prior beliefs, which here, were uniform across all possibilities). 


Walk yourself back through `sampleFromPosterior` function. 

What is happening at each line? 


We can ask things like: Between what 2 numbers are we 95% sure the true proportion lies? This is what's called a *credible interval*. (Note that credible intervals carry a different interpretation than that assigned to traditional confidence intervals. Confidence intervals have a substantially more complicated interpretation, which are about the *procedure of generating a confidence interval*, not about the true population mean. We will discuss the differences more fully later in this course.)

Now that we have the posterior distrbution, we can simply draw some lines in it: Let's look at the points at which 2.5% of the posterior probability mass is to the left and 2.5% of the probability mass is to the right, as well as the point at which 50% of the distribution is to the left and 50% is to the right (i.e., the median).

```{r}
quantile(posterior.Without.NAs, probs = c(0.025, 0.5, 0.975))
```

There are other questions you can ask about the posterior, some of which are highlighted in Wagenmakers, Morey, and Lee (2016) *Bayesian benefits for the pragmatic researcher*.

To conclude this tutorial, I just want to tell you that doing this Bayesian inference trick (the 4 steps outlined above) is common to all of Bayesian analyses. It is the mathematically correct way to use the data you have observed to go from prior beliefs to posterior beliefs.

Because we do it so often, languages like WebPPL have built-in ways to `observe` data. All you need to do is specify the prior, and the likelihood (or linking function; here, the `Binomial`). We then tell the language to do Bayesian inference, and it will return to us a posterior distribution.

Here is how it looks.

```{r}
myPosterior <- "
var ourModel = function(){
  var p = sample(Uniform({a:0, b:1}))
  observe({ data : 15,  link : Binomial( {n:20, p: p} ) })
  return p
}
"
posterior <- webppl(
  program_code = myPosterior,
  model_var = "ourModel",
  inference_opts = list(method = "rejection", samples = 10000),
  packages = c("./utils"),
  output_format = "samples"
)
```

This model will do exactly the same thing we did before. The only difference is that we've abstracted the if `?` statement and the  `repeat(10000,..)`away. We also no longer are explicitly generating the `outcome` variable. 

Instead, we're calling a function called `observe`, which we're giving our `data: 15` and how we would generate the data (i.e, the linking function from our prior beliefs to the data) `link: Binomial(...)`. We then ask the model to `return p`.

This abstraction is very useful, and one of the main reasons for developing languages like WebPPL. With our R call to `webppl`, we use the `inference_opts` argument to tell it to do Rejection Sampling with 10000 samples. We also tell it to do it on `ourModel` using the `model_var` argument.

The `observe` function is a helper function that we've written; we are accessing it through the package `utils`, which is also included in the webppl program call. 

Finally, we've told webppl we want the ouput format in `"samples"` (the default is for a probability table, which we saw in `generative-models.Rmd`). Since we have samples, we can make a histogram.

```{r}
head(posterior)


ggplot(posterior, aes(x = support)) + 
  geom_histogram()
```

Using the same model, change `n` to some different values. (Try 15, 30, 100). 

```{r}
myPosterior <- "
var ourModel = function(){
  var p = sample(Uniform({a:0, b:1}))
  observe({ data : 15,  link : Binomial( {n: ... , p: p} ) })
  return p
}
"

posterior <- webppl(
  program_code = myPosterior,
  model_var = "ourModel",
  inference_opts = list(method = "rejection", samples = 10000),
  packages = c("./utils"),
  output_format = "samples"
)


ggplot(posterior, aes(x = support)) + 
  geom_histogram()

```

What is happening to the posterior in each case? 


Why?



