---
title: "ch6"
author: "mht"
date: "October 21, 2014"
output: html_document
---
# Setup 

```{r}
library(rwebppl)
library(dplyr)
library(tidyr)
library(ggplot2)

getwd()
setwd("~/Repos/psych201s/practicums/")
```

# 6.5 Assessment of malingering

Armed with the knowledge from the previous sections, we now consider the practical challenge of detecting if people cheat on a test. For example, people who have been in a car accident may seek financial compensation from insurance companies by feigning cognitive impairment such as pronounced memory loss. When these people are confronted with a memory test that is intended to measure the extent of their impairment, they may deliberately under-perform. This behavior is called malingering, and it may be accompanied by performance much worse than that displayed by real amnesiacs. Sometimes, for example, malingerers may perform substantially below chance.

Malingering is not, however, always easy to detect, but is naturally addressed by latent-mixture modeling. Using this approach, it is possible to infer which of two categories—those who malinger, and those who are truthful or bona fide—each person belongs to, and quantify the confidence in each of these classifications.

```{r 6.5.1}
malingeringModel <- '
var data = [45, 45, 44, 45, 44, 45, 45,
            45, 45, 45, 30, 20,  6, 44, 
            44, 27, 25, 17, 14, 27, 35, 30];
var n_people = data.length;
var n_questions = 45;

var sampleGroup = function() { return flip(0.5) ? "bonafide" : "malingerer" }

var model = function() {
  var bonaFideRate = uniform(0.5,1)
  var malingererRate = uniform(0,bonaFideRate)
  var personAssignments = repeat(n_people, sampleGroup)

  foreach(utils.range(0, n_people), function(person_id) {
    var pAssignment = personAssignments[person_id]
    var successRate = (pAssignment == "bonafide") ? bonaFideRate : malingererRate
    observe({data: data[person_id], 
             link: Binomial({p: successRate, n: n_questions})})
    query.add("successRate" + person_id, successRate)
  })

  // Add to query table
  query.add("bonaFide", bonaFideRate)
  query.add("malingerer", malingererRate)
  foreach(utils.range(0, n_people), function(i){
    query.add("person_" + i, personAssignments[i])
  })
  return query
}
'

numSamples = 500
wp.samp <- webppl(program_code = malingeringModel,
       inference_opts = list(method="MCMC",
                             kernel =
                               list(HMC =
                                      list(steps = 25,
                                           stepSize = 0.001)),
                          samples = numSamples,
                          burn = numSamples/2,
                          verbose = TRUE),
      model_var = "model",
      packages = c("./utils"),
      output_format = "samples"
      )

# Plot alpha/beta
wp.samp %>% 
  select(bonaFide, malingerer) %>% 
  gather(parameter, value) %>%
  ggplot(aes(x= value, fill = parameter)) + 
  geom_histogram() +
  theme_bw()

# Plot per-person params
wp.samp %>%
  select(starts_with("person")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = paste("person",0:21, sep = "_"))) %>%
  ggplot(., aes(x= value, fill = value)) + 
  geom_bar()+
  facet_wrap(~parameter) +
  theme_bw()

```

Because this was an experimental study, we know that the first 10 participants were bona fide and the next 12 were instructed to malinger.

### Exercise 6.5.1 

What are your conclusions about group membership? Did all of the participants follow the instructions?


# 6.6 Individual differences in malingering

As before, it may seem restrictive to assume that all members of a group have the same chance of answering correctly. So, now we assume that the ith participant in each group has a unique rate of answering questions correctly, θi, which is constrained by group-level distributions. In Section 6.2, we used group-level Gaussians. The problem with that approach is that values can lie outside the range 0 to 1. These values were just censored in Section 6.2, but this is not quite technically correct, and is certainly not elegant.

One of several alternatives is to assume that instead of being Gaussian, the group-level distribution is Beta(alpha,beta).

It is useful to transform the alpha and beta parameters from the beta distribution to a group mean mu = alpha/(alpha+beta) and a measure lambda = alpha+beta that can be conceived of as a precision, in the sense that as it increases the variability of the distribution decreases. It is then straightforward to assign uniform priors to both mu b, the group- level mean for the bona fide participants, and mu m, the group-level mean for the malingerers. This assignment does not, however, reflect our knowledge that mu b > mu m. To capture this knowledge, we could define dunif(0,mubon), as done in the previous model.

However, for this model we apply a different approach. We first define μm as the additive combination of μb and a difference parameter, so that logit(μm) = logit(μb) − μd. Note that this is an additive combination on the logit scale, as is customary in beta-binomial models. The logit transformation is defined as logit(θ) ≡ ln(θ/(1 − θ)) and it transforms values on the rate scale, ranging from 0 to 1, to values on the logit scale, ranging from −∞ to ∞.

```{r 6.6.1}
indDiffMalingeringModel <- '
var data = [45, 45, 44, 45, 44, 45, 45,
            45, 45, 45, 30, 20,  6, 44, 
            44, 27, 25, 17, 14, 27, 35, 30];
var n_people = data.length;
var n_questions = 45;

var positiveGaussian = function(mu, sigma) {
  var gaussianSample = gaussian(mu, sigma)
  factor(gaussianSample > 0 ? gaussianSample : -Infinity)
  return gaussianSample
}

// In this problem, we use a reparameterization of the beta distribution
// defined in terms of its mean and concentration. This is a helper function
// that lets us easily sample from that reparameterized distribution
var beta_mu_lambda = function(mean, concentration) {
  return beta(mean*concentration, (1-mean)*concentration);
}

// Subtracts mu_d from mu_b in logit space, and transforms back to [0,1] 
// logit(mu_mal) = logit(mu_b) - mu_d ==> mu_mal = 1/(1 + e^{-logit(mu_b) + mu_d})
var calcMuMal = function(mu_b, mu_d) {
  var ratio = (1 - mu_b) / mu_b
  return 1 / (1 + ratio * Math.exp(mu_d));
}

var model = function() {
  // Assign groups with baserate phi
  var phi = beta(5,5) 
  var sampleGroup = function() { return flip(phi) ? "bonafide" : "malingerer" }
  var personAssignments = repeat(n_people, sampleGroup)
  
  // Sample a mean and concentration for the bonefides 
  var muBon = beta(1,1)
  var lambdaBon = uniform(40, 800)
  
  // Sample a distance away from muBon
  var muDiff = positiveGaussian(0, 0.5)
  
  // Sample a mean and concentration for the malingerers 
  // (we get mu by subtracting distance in logit space and transforming back to [0,1])
  var muMal = calcMuMal(muBon, muDiff)
  var lambdaMal = uniform(4, 100)
  
  foreach(utils.range(0, n_people), function(person_id) {
    var pAssignment = personAssignments[person_id]
    var successRate = (pAssignment == "bonafide")  ? 
                       beta_mu_lambda(muBon, lambdaBon) :
                       beta_mu_lambda(muMal, lambdaMal);
    observe({data: data[person_id], 
             link: Binomial({p: successRate, n: n_questions})})
    query.add("successRate" + person_id, successRate)
  })

  // Add to query table
  query.add("phi", phi)
  query.add("muMal", muMal)
  query.add("muBon", muBon)
  foreach(utils.range(0, n_people), function(i){
    query.add("person_" + i, personAssignments[i])
  })
  return query
}
'


numSamples = 1800
wp.samp <- webppl(program_code = indDiffMalingeringModel,
       inference_opts = list(method="MCMC",
                             kernel =
                               list(HMC =
                                      list(steps = 20,
                                           stepSize = 0.00075)),
                          samples = numSamples,
                          burn = numSamples/2,
                          verbose = TRUE),
     chains = 2,
     cores = 2,
      model_var = "model",
      packages = c("./utils"),
      output_format = "samples"
      )

# Plot phi
wp.samp %>% 
  select(phi) %>% 
  ggplot(aes(x= phi)) + 
  geom_histogram() +
  theme_bw()

# Plot alpha/beta
wp.samp %>% 
  select(muMal, muBon) %>% 
  gather(parameter, value) %>%
  ggplot(aes(x= value, fill = parameter)) + 
  geom_histogram() +
  theme_bw()

# Plot per-person params
wp.samp %>%
  select(starts_with("person")) %>%
  gather(parameter, value) %>%
  mutate(parameter = factor(parameter,
                            levels = paste("person",0:21, sep = "_"))) %>%
  ggplot(., aes(x= value, fill = value)) + 
  geom_bar()+
  facet_wrap(~parameter) +
  theme_bw()
```

### Exercise 6.6.1 

Is the inferred rate of malingering consistent with what is known
about the instructions given to participants?

### Exercise 6.6.2 

#### Assume you know that the base rate of malingering is 10%. Change the WinBUGS script to reflect this knowledge. Do you expect any differences?

What differences do we expect? If we know the base rate of malingering is 10%, then many more people will be classified as bona fide. If their performance is *malingering* performance, then the bona fide group mean will be lower. What does the bona fide group mean look like?

What else might we suspect? Well, if the bona fide group now includes some people classified as malingerers, the variance (or precision) might increase for the bona fide group.  What does the bona fide group precision look like?

We might also suspect that the mean for the malingerer group **won't** change as much, and the precision might go up a little.

```{r 6.6.2a, echo=FALSE}

```


```{r 6.6.2}

```

### Exercise 6.6.3 

Assume you know for certain that participants 1, 2, and 3 are bona
fide. Change the code to reflect this knowledge.

```{r 6.6.2}

```

### Exercise 6.6.5

Suppose you add a new participant. What number of questions
answered correctly by this participant would lead to the greatest uncertainty
about their group membership?

### Exercise 6.6.6 (skip)

### Exercise 6.6.7

Why are the priors for lambda bon and lambda mal different?

# 6.7 Alzheimer's recall test cheating

Don't worry about running this model, just read the section and answer these high level questions!

### Exercise 6.7.1

Suppose the utilities are very different, so that a false alarm costs $100, because of the risk of litigation in a false accusation, but misses are relatively harmless, costing $10 in wasted administrative costs. What decisions should be made about bona fide and cheating people now?

### Exercise 6.7.2

What other potential information, besides the uncertainty about classification, does the model provide? Give at least one concrete example.

